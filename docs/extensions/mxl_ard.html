
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Mixed logit with ARD &#8212; PyDCML documentation</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Neural networks in utilities" href="nnet_utility.html" />
    <link rel="prev" title="Extensions" href="../extensions.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">PyDCML documentation</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction to PyDCML
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../usage.html">
   Basic usage
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../demos.html">
   Demos
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../demos/mxl-sim500.html">
     Mixed logit with simulated data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../demos/mxl-toyota.html">
     Mixed logit with Toyota data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../demos/avi-sim10000.html">
     Amortized variational inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../understanding.html">
   How PyDCML works
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../extending.html">
   Extending PyDCML
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../extensions.html">
   Extensions
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Mixed logit with ARD
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="nnet_utility.html">
     Neural networks in utilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ordered_logit.html">
     Mixed ordered logit
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/extensions/mxl_ard.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/fmpr/pyDCML"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/fmpr/pyDCML/issues/new?title=Issue%20on%20page%20%2Fextensions/mxl_ard.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/fmpr/pyDCML/master?urlpath=tree/docs/extensions/mxl_ard.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generate-simulated-data">
   Generate simulated data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mixed-logit-specification">
   Mixed Logit specification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-mixed-logit-model-with-automatic-relevance-determination-ard-in-pytorch">
   Bayesian Mixed Logit Model with Automatic Relevance Determination (ARD) in PyTorch
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation-details">
   Implementation details
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Mixed logit with ARD</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generate-simulated-data">
   Generate simulated data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mixed-logit-specification">
   Mixed Logit specification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-mixed-logit-model-with-automatic-relevance-determination-ard-in-pytorch">
   Bayesian Mixed Logit Model with Automatic Relevance Determination (ARD) in PyTorch
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation-details">
   Implementation details
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="mixed-logit-with-ard">
<h1>Mixed logit with ARD<a class="headerlink" href="#mixed-logit-with-ard" title="Permalink to this headline">¶</a></h1>
<p>We begin by performing the necessary imports:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span><span class="o">=</span><span class="s2">&quot;0&quot;</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;/home/rodr/code/amortized-mxl-dev/release&quot;</span><span class="p">)</span> 

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Fix random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="generate-simulated-data">
<h2>Generate simulated data<a class="headerlink" href="#generate-simulated-data" title="Permalink to this headline">¶</a></h2>
<p>For this demo, we generate simulated choice data, but we will set one of the fixed effects parameters and one of the random effects parameters to zero, such that their associatied attributes are irrelevant for the utilities.</p>
<p>Other that that, the simulated data is generated in a similar way to the “Mixed logit with simulated data” demo. We predefine the fixed effects parameters (true_alpha) and random effects parameters (true_beta), as well as the covariance matrix (true_Omega), and sample simulated choice data for 2000 respondents (num_resp), each with 5 choice situations (num_menus). The number of choice alternatives is set to 5.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">core.dcm_fakedata</span> <span class="kn">import</span> <span class="n">generate_fake_data_wide</span>

<span class="n">num_resp</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">num_menus</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">num_alternatives</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># one of the parameters is set to zero such that the corresponding input variables  </span>
<span class="c1"># is not correlated with the observed choices</span>
<span class="n">true_alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> 
<span class="n">true_beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="c1"># dynamic version of generating Omega</span>
<span class="n">corr</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">scale_factor</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">true_Omega</span> <span class="o">=</span> <span class="n">corr</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">true_beta</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">true_beta</span><span class="p">)))</span> <span class="c1"># off-diagonal values of cov matrix</span>
<span class="n">true_Omega</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">true_beta</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">true_beta</span><span class="p">))]</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="c1"># diagonal values of cov matrix</span>
<span class="n">true_Omega</span> <span class="o">*=</span> <span class="n">scale_factor</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">generate_fake_data_wide</span><span class="p">(</span><span class="n">num_resp</span><span class="p">,</span> <span class="n">num_menus</span><span class="p">,</span> <span class="n">num_alternatives</span><span class="p">,</span> <span class="n">true_alpha</span><span class="p">,</span> <span class="n">true_beta</span><span class="p">,</span> <span class="n">true_Omega</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating fake data...
Error: 42.18
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ALT1_XF1</th>
      <th>ALT1_XF2</th>
      <th>ALT1_XF3</th>
      <th>ALT1_XR1</th>
      <th>ALT1_XR2</th>
      <th>ALT1_XR3</th>
      <th>ALT1_XR4</th>
      <th>ALT1_XR5</th>
      <th>ALT2_XF1</th>
      <th>ALT2_XF2</th>
      <th>...</th>
      <th>ALT5_XR1</th>
      <th>ALT5_XR2</th>
      <th>ALT5_XR3</th>
      <th>ALT5_XR4</th>
      <th>ALT5_XR5</th>
      <th>choice</th>
      <th>indID</th>
      <th>menuID</th>
      <th>obsID</th>
      <th>ones</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.374540</td>
      <td>0.950714</td>
      <td>0.731994</td>
      <td>0.069212</td>
      <td>0.585697</td>
      <td>0.798869</td>
      <td>0.764473</td>
      <td>0.837396</td>
      <td>0.598658</td>
      <td>0.156019</td>
      <td>...</td>
      <td>0.119691</td>
      <td>0.938057</td>
      <td>0.510089</td>
      <td>0.300522</td>
      <td>0.624496</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.183405</td>
      <td>0.304242</td>
      <td>0.524756</td>
      <td>0.257912</td>
      <td>0.040451</td>
      <td>0.793656</td>
      <td>0.995865</td>
      <td>0.956444</td>
      <td>0.431945</td>
      <td>0.291229</td>
      <td>...</td>
      <td>0.107102</td>
      <td>0.495835</td>
      <td>0.287860</td>
      <td>0.651430</td>
      <td>0.145982</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.607545</td>
      <td>0.170524</td>
      <td>0.065052</td>
      <td>0.588614</td>
      <td>0.571589</td>
      <td>0.402953</td>
      <td>0.482491</td>
      <td>0.146977</td>
      <td>0.948886</td>
      <td>0.965632</td>
      <td>...</td>
      <td>0.685591</td>
      <td>0.543384</td>
      <td>0.531436</td>
      <td>0.551352</td>
      <td>0.486627</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.662522</td>
      <td>0.311711</td>
      <td>0.520068</td>
      <td>0.946478</td>
      <td>0.618740</td>
      <td>0.459393</td>
      <td>0.860055</td>
      <td>0.889657</td>
      <td>0.546710</td>
      <td>0.184854</td>
      <td>...</td>
      <td>0.071816</td>
      <td>0.562382</td>
      <td>0.314172</td>
      <td>0.099657</td>
      <td>0.190283</td>
      <td>3</td>
      <td>0</td>
      <td>3</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.388677</td>
      <td>0.271349</td>
      <td>0.828738</td>
      <td>0.790511</td>
      <td>0.806431</td>
      <td>0.767619</td>
      <td>0.075828</td>
      <td>0.110584</td>
      <td>0.356753</td>
      <td>0.280935</td>
      <td>...</td>
      <td>0.174194</td>
      <td>0.348183</td>
      <td>0.348195</td>
      <td>0.824453</td>
      <td>0.496267</td>
      <td>3</td>
      <td>0</td>
      <td>4</td>
      <td>4</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 45 columns</p>
</div></div></div>
</div>
</section>
<section id="mixed-logit-specification">
<h2>Mixed Logit specification<a class="headerlink" href="#mixed-logit-specification" title="Permalink to this headline">¶</a></h2>
<p>We now make use of the developed formula interface to specify the utilities of the mixed logit model.</p>
<p>We begin by defining the fixed effects parameters, the random effects parameters, and the observed variables. This creates instances of Python objects that can be put together to define the utility functions for the different alternatives.</p>
<p>Once the utilities are defined, we collect them in a Python dictionary mapping alternative names to their corresponding expressions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">core.dcm_interface</span> <span class="kn">import</span> <span class="n">FixedEffect</span><span class="p">,</span> <span class="n">RandomEffect</span><span class="p">,</span> <span class="n">ObservedVariable</span>
<span class="kn">import</span> <span class="nn">torch.distributions</span> <span class="k">as</span> <span class="nn">dists</span>

<span class="c1"># define fixed effects parameters</span>
<span class="n">B_XF1</span> <span class="o">=</span> <span class="n">FixedEffect</span><span class="p">(</span><span class="s1">&#39;BETA_XF1&#39;</span><span class="p">)</span>
<span class="n">B_XF2</span> <span class="o">=</span> <span class="n">FixedEffect</span><span class="p">(</span><span class="s1">&#39;BETA_XF2&#39;</span><span class="p">)</span>
<span class="n">B_XF3</span> <span class="o">=</span> <span class="n">FixedEffect</span><span class="p">(</span><span class="s1">&#39;BETA_XF3&#39;</span><span class="p">)</span>

<span class="c1"># define random effects parameters</span>
<span class="n">B_XR1</span> <span class="o">=</span> <span class="n">RandomEffect</span><span class="p">(</span><span class="s1">&#39;BETA_XR1&#39;</span><span class="p">)</span>
<span class="n">B_XR2</span> <span class="o">=</span> <span class="n">RandomEffect</span><span class="p">(</span><span class="s1">&#39;BETA_XR2&#39;</span><span class="p">)</span>
<span class="n">B_XR3</span> <span class="o">=</span> <span class="n">RandomEffect</span><span class="p">(</span><span class="s1">&#39;BETA_XR3&#39;</span><span class="p">)</span>
<span class="n">B_XR4</span> <span class="o">=</span> <span class="n">RandomEffect</span><span class="p">(</span><span class="s1">&#39;BETA_XR4&#39;</span><span class="p">)</span>
<span class="n">B_XR5</span> <span class="o">=</span> <span class="n">RandomEffect</span><span class="p">(</span><span class="s1">&#39;BETA_XR5&#39;</span><span class="p">)</span>

<span class="c1"># define observed variables</span>
<span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">exec</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> = ObservedVariable(&#39;</span><span class="si">%s</span><span class="s2">&#39;)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">attr</span><span class="p">,</span><span class="n">attr</span><span class="p">))</span>

<span class="c1"># define utility functions</span>
<span class="n">V1</span> <span class="o">=</span> <span class="n">B_XF1</span><span class="o">*</span><span class="n">ALT1_XF1</span> <span class="o">+</span> <span class="n">B_XF2</span><span class="o">*</span><span class="n">ALT1_XF2</span> <span class="o">+</span> <span class="n">B_XF3</span><span class="o">*</span><span class="n">ALT1_XF3</span> <span class="o">+</span> <span class="n">B_XR1</span><span class="o">*</span><span class="n">ALT1_XR1</span> <span class="o">+</span> <span class="n">B_XR2</span><span class="o">*</span><span class="n">ALT1_XR2</span> <span class="o">+</span> <span class="n">B_XR3</span><span class="o">*</span><span class="n">ALT1_XR3</span> <span class="o">+</span> <span class="n">B_XR4</span><span class="o">*</span><span class="n">ALT1_XR4</span> <span class="o">+</span> <span class="n">B_XR5</span><span class="o">*</span><span class="n">ALT1_XR5</span>
<span class="n">V2</span> <span class="o">=</span> <span class="n">B_XF1</span><span class="o">*</span><span class="n">ALT2_XF1</span> <span class="o">+</span> <span class="n">B_XF2</span><span class="o">*</span><span class="n">ALT2_XF2</span> <span class="o">+</span> <span class="n">B_XF3</span><span class="o">*</span><span class="n">ALT2_XF3</span> <span class="o">+</span> <span class="n">B_XR1</span><span class="o">*</span><span class="n">ALT2_XR1</span> <span class="o">+</span> <span class="n">B_XR2</span><span class="o">*</span><span class="n">ALT2_XR2</span> <span class="o">+</span> <span class="n">B_XR3</span><span class="o">*</span><span class="n">ALT2_XR3</span> <span class="o">+</span> <span class="n">B_XR4</span><span class="o">*</span><span class="n">ALT2_XR4</span> <span class="o">+</span> <span class="n">B_XR5</span><span class="o">*</span><span class="n">ALT2_XR5</span>
<span class="n">V3</span> <span class="o">=</span> <span class="n">B_XF1</span><span class="o">*</span><span class="n">ALT3_XF1</span> <span class="o">+</span> <span class="n">B_XF2</span><span class="o">*</span><span class="n">ALT3_XF2</span> <span class="o">+</span> <span class="n">B_XF3</span><span class="o">*</span><span class="n">ALT3_XF3</span> <span class="o">+</span> <span class="n">B_XR1</span><span class="o">*</span><span class="n">ALT3_XR1</span> <span class="o">+</span> <span class="n">B_XR2</span><span class="o">*</span><span class="n">ALT3_XR2</span> <span class="o">+</span> <span class="n">B_XR3</span><span class="o">*</span><span class="n">ALT3_XR3</span> <span class="o">+</span> <span class="n">B_XR4</span><span class="o">*</span><span class="n">ALT3_XR4</span> <span class="o">+</span> <span class="n">B_XR5</span><span class="o">*</span><span class="n">ALT3_XR5</span>
<span class="n">V4</span> <span class="o">=</span> <span class="n">B_XF1</span><span class="o">*</span><span class="n">ALT4_XF1</span> <span class="o">+</span> <span class="n">B_XF2</span><span class="o">*</span><span class="n">ALT4_XF2</span> <span class="o">+</span> <span class="n">B_XF3</span><span class="o">*</span><span class="n">ALT4_XF3</span> <span class="o">+</span> <span class="n">B_XR1</span><span class="o">*</span><span class="n">ALT4_XR1</span> <span class="o">+</span> <span class="n">B_XR2</span><span class="o">*</span><span class="n">ALT4_XR2</span> <span class="o">+</span> <span class="n">B_XR3</span><span class="o">*</span><span class="n">ALT4_XR3</span> <span class="o">+</span> <span class="n">B_XR4</span><span class="o">*</span><span class="n">ALT4_XR4</span> <span class="o">+</span> <span class="n">B_XR5</span><span class="o">*</span><span class="n">ALT4_XR5</span>
<span class="n">V5</span> <span class="o">=</span> <span class="n">B_XF1</span><span class="o">*</span><span class="n">ALT5_XF1</span> <span class="o">+</span> <span class="n">B_XF2</span><span class="o">*</span><span class="n">ALT5_XF2</span> <span class="o">+</span> <span class="n">B_XF3</span><span class="o">*</span><span class="n">ALT5_XF3</span> <span class="o">+</span> <span class="n">B_XR1</span><span class="o">*</span><span class="n">ALT5_XR1</span> <span class="o">+</span> <span class="n">B_XR2</span><span class="o">*</span><span class="n">ALT5_XR2</span> <span class="o">+</span> <span class="n">B_XR3</span><span class="o">*</span><span class="n">ALT5_XR3</span> <span class="o">+</span> <span class="n">B_XR4</span><span class="o">*</span><span class="n">ALT5_XR4</span> <span class="o">+</span> <span class="n">B_XR5</span><span class="o">*</span><span class="n">ALT5_XR5</span>

<span class="c1"># associate utility functions with the names of the alternatives</span>
<span class="n">utilities</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;ALT1&quot;</span><span class="p">:</span> <span class="n">V1</span><span class="p">,</span> <span class="s2">&quot;ALT2&quot;</span><span class="p">:</span> <span class="n">V2</span><span class="p">,</span> <span class="s2">&quot;ALT3&quot;</span><span class="p">:</span> <span class="n">V3</span><span class="p">,</span> <span class="s2">&quot;ALT4&quot;</span><span class="p">:</span> <span class="n">V4</span><span class="p">,</span> <span class="s2">&quot;ALT5&quot;</span><span class="p">:</span> <span class="n">V5</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>We are now ready to create a Specification object containing the utilities that we have just defined. Note that we must also specify the type of choice model to be used - a mixed logit model (MXL) in this case.</p>
<p>Note that we can inspect the specification by printing the dcm_spec object.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">core.dcm_interface</span> <span class="kn">import</span> <span class="n">Specification</span>

<span class="c1"># create MXL specification object based on the utilities previously defined</span>
<span class="n">dcm_spec</span> <span class="o">=</span> <span class="n">Specification</span><span class="p">(</span><span class="s1">&#39;MXL&#39;</span><span class="p">,</span> <span class="n">utilities</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dcm_spec</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>----------------- MXL specification:
Alternatives: [&#39;ALT1&#39;, &#39;ALT2&#39;, &#39;ALT3&#39;, &#39;ALT4&#39;, &#39;ALT5&#39;]
Utility functions:
   V_ALT1 = BETA_XF1*ALT1_XF1 + BETA_XF2*ALT1_XF2 + BETA_XF3*ALT1_XF3 + BETA_XR1_n*ALT1_XR1 + BETA_XR2_n*ALT1_XR2 + BETA_XR3_n*ALT1_XR3 + BETA_XR4_n*ALT1_XR4 + BETA_XR5_n*ALT1_XR5
   V_ALT2 = BETA_XF1*ALT2_XF1 + BETA_XF2*ALT2_XF2 + BETA_XF3*ALT2_XF3 + BETA_XR1_n*ALT2_XR1 + BETA_XR2_n*ALT2_XR2 + BETA_XR3_n*ALT2_XR3 + BETA_XR4_n*ALT2_XR4 + BETA_XR5_n*ALT2_XR5
   V_ALT3 = BETA_XF1*ALT3_XF1 + BETA_XF2*ALT3_XF2 + BETA_XF3*ALT3_XF3 + BETA_XR1_n*ALT3_XR1 + BETA_XR2_n*ALT3_XR2 + BETA_XR3_n*ALT3_XR3 + BETA_XR4_n*ALT3_XR4 + BETA_XR5_n*ALT3_XR5
   V_ALT4 = BETA_XF1*ALT4_XF1 + BETA_XF2*ALT4_XF2 + BETA_XF3*ALT4_XF3 + BETA_XR1_n*ALT4_XR1 + BETA_XR2_n*ALT4_XR2 + BETA_XR3_n*ALT4_XR3 + BETA_XR4_n*ALT4_XR4 + BETA_XR5_n*ALT4_XR5
   V_ALT5 = BETA_XF1*ALT5_XF1 + BETA_XF2*ALT5_XF2 + BETA_XF3*ALT5_XF3 + BETA_XR1_n*ALT5_XR1 + BETA_XR2_n*ALT5_XR2 + BETA_XR3_n*ALT5_XR3 + BETA_XR4_n*ALT5_XR4 + BETA_XR5_n*ALT5_XR5

Num. parameters to be estimated: 8
Fixed effects params: [&#39;BETA_XF1&#39;, &#39;BETA_XF2&#39;, &#39;BETA_XF3&#39;]
Random effects params: [&#39;BETA_XR1&#39;, &#39;BETA_XR2&#39;, &#39;BETA_XR3&#39;, &#39;BETA_XR4&#39;, &#39;BETA_XR5&#39;]
</pre></div>
</div>
</div>
</div>
<p>Once the Specification is defined, we need to define the DCM Dataset object that goes along with it. For this, we instantiate the Dataset class with the Pandas dataframe containing the data in the so-called “wide format”, the name of column in the dataframe containing the observed choices and the dcm_spec that we have previously created.</p>
<p>Note that since this is panel data, we must also specify the name of the column in the dataframe that contains the ID of the respondent (this should be a integer ranging from 0 the num_resp-1).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">core.dcm_interface</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="c1"># create DCM dataset object</span>
<span class="n">dcm_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s1">&#39;choice&#39;</span><span class="p">,</span> <span class="n">dcm_spec</span><span class="p">,</span> <span class="n">resp_id_col</span><span class="o">=</span><span class="s1">&#39;indID&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Preparing dataset...
	Model type: MXL
	Num. observations: 10000
	Num. alternatives: 5
	Num. respondents: 2000
	Num. menus: 5
	Observations IDs: [   0    1    2 ... 9997 9998 9999]
	Alternative IDs: None
	Respondent IDs: [   0    0    0 ... 1999 1999 1999]
	Availability columns: None
	Attribute names: [&#39;ALT1_XF1&#39;, &#39;ALT1_XF2&#39;, &#39;ALT1_XF3&#39;, &#39;ALT1_XR1&#39;, &#39;ALT1_XR2&#39;, &#39;ALT1_XR3&#39;, &#39;ALT1_XR4&#39;, &#39;ALT1_XR5&#39;, &#39;ALT2_XF1&#39;, &#39;ALT2_XF2&#39;, &#39;ALT2_XF3&#39;, &#39;ALT2_XR1&#39;, &#39;ALT2_XR2&#39;, &#39;ALT2_XR3&#39;, &#39;ALT2_XR4&#39;, &#39;ALT2_XR5&#39;, &#39;ALT3_XF1&#39;, &#39;ALT3_XF2&#39;, &#39;ALT3_XF3&#39;, &#39;ALT3_XR1&#39;, &#39;ALT3_XR2&#39;, &#39;ALT3_XR3&#39;, &#39;ALT3_XR4&#39;, &#39;ALT3_XR5&#39;, &#39;ALT4_XF1&#39;, &#39;ALT4_XF2&#39;, &#39;ALT4_XF3&#39;, &#39;ALT4_XR1&#39;, &#39;ALT4_XR2&#39;, &#39;ALT4_XR3&#39;, &#39;ALT4_XR4&#39;, &#39;ALT4_XR5&#39;, &#39;ALT5_XF1&#39;, &#39;ALT5_XF2&#39;, &#39;ALT5_XF3&#39;, &#39;ALT5_XR1&#39;, &#39;ALT5_XR2&#39;, &#39;ALT5_XR3&#39;, &#39;ALT5_XR4&#39;, &#39;ALT5_XR5&#39;]
	Fixed effects attribute names: [&#39;ALT1_XF1&#39;, &#39;ALT1_XF2&#39;, &#39;ALT1_XF3&#39;, &#39;ALT2_XF1&#39;, &#39;ALT2_XF2&#39;, &#39;ALT2_XF3&#39;, &#39;ALT3_XF1&#39;, &#39;ALT3_XF2&#39;, &#39;ALT3_XF3&#39;, &#39;ALT4_XF1&#39;, &#39;ALT4_XF2&#39;, &#39;ALT4_XF3&#39;, &#39;ALT5_XF1&#39;, &#39;ALT5_XF2&#39;, &#39;ALT5_XF3&#39;]
	Fixed effects parameter names: [&#39;BETA_XF1&#39;, &#39;BETA_XF2&#39;, &#39;BETA_XF3&#39;, &#39;BETA_XF1&#39;, &#39;BETA_XF2&#39;, &#39;BETA_XF3&#39;, &#39;BETA_XF1&#39;, &#39;BETA_XF2&#39;, &#39;BETA_XF3&#39;, &#39;BETA_XF1&#39;, &#39;BETA_XF2&#39;, &#39;BETA_XF3&#39;, &#39;BETA_XF1&#39;, &#39;BETA_XF2&#39;, &#39;BETA_XF3&#39;]
	Random effects attribute names: [&#39;ALT1_XR1&#39;, &#39;ALT1_XR2&#39;, &#39;ALT1_XR3&#39;, &#39;ALT1_XR4&#39;, &#39;ALT1_XR5&#39;, &#39;ALT2_XR1&#39;, &#39;ALT2_XR2&#39;, &#39;ALT2_XR3&#39;, &#39;ALT2_XR4&#39;, &#39;ALT2_XR5&#39;, &#39;ALT3_XR1&#39;, &#39;ALT3_XR2&#39;, &#39;ALT3_XR3&#39;, &#39;ALT3_XR4&#39;, &#39;ALT3_XR5&#39;, &#39;ALT4_XR1&#39;, &#39;ALT4_XR2&#39;, &#39;ALT4_XR3&#39;, &#39;ALT4_XR4&#39;, &#39;ALT4_XR5&#39;, &#39;ALT5_XR1&#39;, &#39;ALT5_XR2&#39;, &#39;ALT5_XR3&#39;, &#39;ALT5_XR4&#39;, &#39;ALT5_XR5&#39;]
	Random effects parameter names: [&#39;BETA_XR1&#39;, &#39;BETA_XR2&#39;, &#39;BETA_XR3&#39;, &#39;BETA_XR4&#39;, &#39;BETA_XR5&#39;, &#39;BETA_XR1&#39;, &#39;BETA_XR2&#39;, &#39;BETA_XR3&#39;, &#39;BETA_XR4&#39;, &#39;BETA_XR5&#39;, &#39;BETA_XR1&#39;, &#39;BETA_XR2&#39;, &#39;BETA_XR3&#39;, &#39;BETA_XR4&#39;, &#39;BETA_XR5&#39;, &#39;BETA_XR1&#39;, &#39;BETA_XR2&#39;, &#39;BETA_XR3&#39;, &#39;BETA_XR4&#39;, &#39;BETA_XR5&#39;, &#39;BETA_XR1&#39;, &#39;BETA_XR2&#39;, &#39;BETA_XR3&#39;, &#39;BETA_XR4&#39;, &#39;BETA_XR5&#39;]
	Alternative attributes ndarray.shape: (2000, 5, 40)
	Choices ndarray.shape: (2000, 5)
	Alternatives availability ndarray.shape: (2000, 5, 5)
	Data mask ndarray.shape: (2000, 5)
	Context data ndarray.shape: (2000, 0)
	Neural nets data ndarray.shape: (2000, 0)
Done!
</pre></div>
</div>
</div>
</div>
<p>As with the specification, we can inspect the DCM dataset by printing the dcm_dataset object:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">dcm_dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>----------------- DCM dataset:
Model type: MXL
Num. observations: 10000
Num. alternatives: 5
Num. respondents: 2000
Num. menus: 5
Num. fixed effects: 15
Num. random effects: 25
Attribute names: [&#39;ALT1_XF1&#39;, &#39;ALT1_XF2&#39;, &#39;ALT1_XF3&#39;, &#39;ALT1_XR1&#39;, &#39;ALT1_XR2&#39;, &#39;ALT1_XR3&#39;, &#39;ALT1_XR4&#39;, &#39;ALT1_XR5&#39;, &#39;ALT2_XF1&#39;, &#39;ALT2_XF2&#39;, &#39;ALT2_XF3&#39;, &#39;ALT2_XR1&#39;, &#39;ALT2_XR2&#39;, &#39;ALT2_XR3&#39;, &#39;ALT2_XR4&#39;, &#39;ALT2_XR5&#39;, &#39;ALT3_XF1&#39;, &#39;ALT3_XF2&#39;, &#39;ALT3_XF3&#39;, &#39;ALT3_XR1&#39;, &#39;ALT3_XR2&#39;, &#39;ALT3_XR3&#39;, &#39;ALT3_XR4&#39;, &#39;ALT3_XR5&#39;, &#39;ALT4_XF1&#39;, &#39;ALT4_XF2&#39;, &#39;ALT4_XF3&#39;, &#39;ALT4_XR1&#39;, &#39;ALT4_XR2&#39;, &#39;ALT4_XR3&#39;, &#39;ALT4_XR4&#39;, &#39;ALT4_XR5&#39;, &#39;ALT5_XF1&#39;, &#39;ALT5_XF2&#39;, &#39;ALT5_XF3&#39;, &#39;ALT5_XR1&#39;, &#39;ALT5_XR2&#39;, &#39;ALT5_XR3&#39;, &#39;ALT5_XR4&#39;, &#39;ALT5_XR5&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="bayesian-mixed-logit-model-with-automatic-relevance-determination-ard-in-pytorch">
<h2>Bayesian Mixed Logit Model with Automatic Relevance Determination (ARD) in PyTorch<a class="headerlink" href="#bayesian-mixed-logit-model-with-automatic-relevance-determination-ard-in-pytorch" title="Permalink to this headline">¶</a></h2>
<p>We will perform ARD in the MXL model in a similar way as proposed in: Rodrigues, F., Ortelli, N., Bierlaire, M. and Pereira, F.C., 2020. Bayesian automatic relevance determination for utility function specification in discrete choice models. IEEE Transactions on Intelligent Transportation Systems.</p>
<p>We begin by modifying the generative process of the core MXL model as follows (changes to the MXL model are highlighted in red):
<span class="math notranslate nohighlight">\(\require{color}\)</span></p>
<ol class="simple">
<li><p><font color='red'> Draw variance for prior over fixed taste parameters <span class="math notranslate nohighlight">\(\boldsymbol\tau_\alpha \sim \mbox{InverseGamma}(\alpha_0, \beta_0)\)</span> </font></p></li>
<li><p>Draw fixed taste parameters <span class="math notranslate nohighlight">\(\boldsymbol\alpha \sim \mathcal{N}(\boldsymbol\lambda_0, \color{red} \mbox{diag}(\boldsymbol\tau_\zeta) \color{black})\)</span></p></li>
<li><p><font color='red'> Draw variance for prior over random taste parameters <span class="math notranslate nohighlight">\(\boldsymbol\tau_\zeta \sim \mbox{InverseGamma}(\alpha_0, \beta_0)\)</span> </font></p></li>
<li><p>Draw mean vector <span class="math notranslate nohighlight">\(\boldsymbol\zeta \sim \mathcal{N}(\boldsymbol\mu_0, \color{red} \mbox{diag}(\boldsymbol\tau_\zeta) \color{black})\)</span></p></li>
<li><p>Draw scales vector <span class="math notranslate nohighlight">\(\boldsymbol\theta \sim \mbox{half-Cauchy}(\boldsymbol\sigma_0)\)</span></p></li>
<li><p>Draw correlation matrix <span class="math notranslate nohighlight">\(\boldsymbol\Psi \sim \mbox{LKJ}(\nu)\)</span></p></li>
<li><p>For each decision-maker <span class="math notranslate nohighlight">\(n \in \{1,\dots,N\}\)</span></p>
<ol class="simple">
<li><p>Draw random taste parameters <span class="math notranslate nohighlight">\(\boldsymbol\beta_n \sim \mathcal{N}(\boldsymbol\zeta,\boldsymbol\Omega)\)</span></p></li>
<li><p>For each choice occasion <span class="math notranslate nohighlight">\(t \in \{1,\dots,T_n\}\)</span></p>
<ol class="simple">
<li><p>Draw observed choice <span class="math notranslate nohighlight">\(y_{nt} \sim \mbox{MNL}(\boldsymbol\alpha, \boldsymbol\beta_n, \textbf{X}_{nt})\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol\Omega = \mbox{diag}(\boldsymbol\theta) \times \boldsymbol\Psi \times  \mbox{diag}(\boldsymbol\theta)\)</span>.</p>
<p>This model is already implemented in the class <code class="docutils literal notranslate"><span class="pre">TorchMXL_ARD</span></code>. At the end of this notebook, we provide an explanation of how this extension was implemented.</p>
<p>We can instantiate this model from the TorchMXL_ARD using the following code. We can the run variational inference to approximate the posterior distribution of the latent variables in the model. Note that since in this case we know the true parameters that were used to generate the simualated choice data, we can pass them to the “infer” method in order to obtain additional information during the ELBO maximization (useful for tracking the progress of VI and for other debugging purposes).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>

<span class="kn">from</span> <span class="nn">core.torch_mxl_ard</span> <span class="kn">import</span> <span class="n">TorchMXL_ARD</span>

<span class="c1"># instantiate MXL model</span>
<span class="n">mxl</span> <span class="o">=</span> <span class="n">TorchMXL_ARD</span><span class="p">(</span><span class="n">dcm_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">num_resp</span><span class="p">,</span> <span class="n">use_inference_net</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_cuda</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># run Bayesian inference (variational inference)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">mxl</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">true_alpha</span><span class="o">=</span><span class="n">true_alpha</span><span class="p">,</span> <span class="n">true_beta</span><span class="o">=</span><span class="n">true_beta</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Epoch     0] ELBO: 30651; Loglik: -21061; Acc.: 0.169; Alpha RMSE: 0.645; Beta RMSE: 0.765
[Epoch   100] ELBO: 36925; Loglik: -16834; Acc.: 0.277; Alpha RMSE: 0.265; Beta RMSE: 0.788
[Epoch   200] ELBO: 28090; Loglik: -15822; Acc.: 0.326; Alpha RMSE: 0.091; Beta RMSE: 0.773
[Epoch   300] ELBO: 18901; Loglik: -14839; Acc.: 0.376; Alpha RMSE: 0.048; Beta RMSE: 0.725
[Epoch   400] ELBO: 19277; Loglik: -14460; Acc.: 0.388; Alpha RMSE: 0.086; Beta RMSE: 0.699
[Epoch   500] ELBO: 18957; Loglik: -13972; Acc.: 0.414; Alpha RMSE: 0.082; Beta RMSE: 0.623
[Epoch   600] ELBO: 17863; Loglik: -13922; Acc.: 0.417; Alpha RMSE: 0.068; Beta RMSE: 0.594
[Epoch   700] ELBO: 16851; Loglik: -13622; Acc.: 0.436; Alpha RMSE: 0.073; Beta RMSE: 0.549
[Epoch   800] ELBO: 16725; Loglik: -13468; Acc.: 0.443; Alpha RMSE: 0.124; Beta RMSE: 0.502
[Epoch   900] ELBO: 16681; Loglik: -13089; Acc.: 0.458; Alpha RMSE: 0.101; Beta RMSE: 0.446
[Epoch  1000] ELBO: 16433; Loglik: -13137; Acc.: 0.463; Alpha RMSE: 0.117; Beta RMSE: 0.407
[Epoch  1100] ELBO: 16039; Loglik: -12896; Acc.: 0.475; Alpha RMSE: 0.132; Beta RMSE: 0.357
[Epoch  1200] ELBO: 16181; Loglik: -12948; Acc.: 0.467; Alpha RMSE: 0.134; Beta RMSE: 0.318
[Epoch  1300] ELBO: 16435; Loglik: -13016; Acc.: 0.466; Alpha RMSE: 0.134; Beta RMSE: 0.291
[Epoch  1400] ELBO: 15930; Loglik: -12756; Acc.: 0.473; Alpha RMSE: 0.153; Beta RMSE: 0.253
[Epoch  1500] ELBO: 15771; Loglik: -12708; Acc.: 0.480; Alpha RMSE: 0.150; Beta RMSE: 0.223
[Epoch  1600] ELBO: 15783; Loglik: -12719; Acc.: 0.483; Alpha RMSE: 0.152; Beta RMSE: 0.201
[Epoch  1700] ELBO: 15770; Loglik: -12713; Acc.: 0.475; Alpha RMSE: 0.150; Beta RMSE: 0.156
[Epoch  1800] ELBO: 15704; Loglik: -12737; Acc.: 0.477; Alpha RMSE: 0.144; Beta RMSE: 0.127
[Epoch  1900] ELBO: 15666; Loglik: -12626; Acc.: 0.485; Alpha RMSE: 0.159; Beta RMSE: 0.090
[Epoch  2000] ELBO: 15738; Loglik: -12645; Acc.: 0.488; Alpha RMSE: 0.164; Beta RMSE: 0.082
[Epoch  2100] ELBO: 15656; Loglik: -12618; Acc.: 0.486; Alpha RMSE: 0.154; Beta RMSE: 0.084
[Epoch  2200] ELBO: 15617; Loglik: -12564; Acc.: 0.482; Alpha RMSE: 0.165; Beta RMSE: 0.081
[Epoch  2300] ELBO: 15517; Loglik: -12605; Acc.: 0.484; Alpha RMSE: 0.161; Beta RMSE: 0.083
[Epoch  2400] ELBO: 15389; Loglik: -12629; Acc.: 0.488; Alpha RMSE: 0.149; Beta RMSE: 0.084
[Epoch  2500] ELBO: 15326; Loglik: -12661; Acc.: 0.488; Alpha RMSE: 0.161; Beta RMSE: 0.092
[Epoch  2600] ELBO: 15370; Loglik: -12631; Acc.: 0.484; Alpha RMSE: 0.139; Beta RMSE: 0.102
[Epoch  2700] ELBO: 15366; Loglik: -12793; Acc.: 0.484; Alpha RMSE: 0.135; Beta RMSE: 0.115
[Epoch  2800] ELBO: 15255; Loglik: -12647; Acc.: 0.483; Alpha RMSE: 0.152; Beta RMSE: 0.117
[Epoch  2900] ELBO: 15538; Loglik: -12722; Acc.: 0.478; Alpha RMSE: 0.148; Beta RMSE: 0.100
[Epoch  3000] ELBO: 15273; Loglik: -12792; Acc.: 0.478; Alpha RMSE: 0.149; Beta RMSE: 0.122
[Epoch  3100] ELBO: 15169; Loglik: -12714; Acc.: 0.484; Alpha RMSE: 0.142; Beta RMSE: 0.130
[Epoch  3200] ELBO: 15295; Loglik: -12820; Acc.: 0.481; Alpha RMSE: 0.133; Beta RMSE: 0.124
[Epoch  3300] ELBO: 15447; Loglik: -12823; Acc.: 0.476; Alpha RMSE: 0.137; Beta RMSE: 0.129
[Epoch  3400] ELBO: 15348; Loglik: -12769; Acc.: 0.475; Alpha RMSE: 0.128; Beta RMSE: 0.123
[Epoch  3500] ELBO: 15313; Loglik: -12796; Acc.: 0.471; Alpha RMSE: 0.121; Beta RMSE: 0.132
[Epoch  3600] ELBO: 15236; Loglik: -12797; Acc.: 0.473; Alpha RMSE: 0.130; Beta RMSE: 0.116
[Epoch  3700] ELBO: 15085; Loglik: -12827; Acc.: 0.473; Alpha RMSE: 0.131; Beta RMSE: 0.120
[Epoch  3800] ELBO: 15123; Loglik: -12819; Acc.: 0.475; Alpha RMSE: 0.131; Beta RMSE: 0.109
[Epoch  3900] ELBO: 15199; Loglik: -12880; Acc.: 0.474; Alpha RMSE: 0.126; Beta RMSE: 0.120
[Epoch  4000] ELBO: 15179; Loglik: -12964; Acc.: 0.464; Alpha RMSE: 0.125; Beta RMSE: 0.104
[Epoch  4100] ELBO: 15123; Loglik: -12944; Acc.: 0.460; Alpha RMSE: 0.130; Beta RMSE: 0.110
[Epoch  4200] ELBO: 15139; Loglik: -12957; Acc.: 0.469; Alpha RMSE: 0.115; Beta RMSE: 0.099
[Epoch  4300] ELBO: 14954; Loglik: -12867; Acc.: 0.468; Alpha RMSE: 0.113; Beta RMSE: 0.115
[Epoch  4400] ELBO: 15007; Loglik: -12925; Acc.: 0.467; Alpha RMSE: 0.120; Beta RMSE: 0.094
[Epoch  4500] ELBO: 15044; Loglik: -12987; Acc.: 0.462; Alpha RMSE: 0.110; Beta RMSE: 0.106
[Epoch  4600] ELBO: 15055; Loglik: -13000; Acc.: 0.461; Alpha RMSE: 0.111; Beta RMSE: 0.091
[Epoch  4700] ELBO: 14921; Loglik: -12953; Acc.: 0.468; Alpha RMSE: 0.099; Beta RMSE: 0.087
[Epoch  4800] ELBO: 14981; Loglik: -12959; Acc.: 0.463; Alpha RMSE: 0.112; Beta RMSE: 0.084
[Epoch  4900] ELBO: 15000; Loglik: -12967; Acc.: 0.460; Alpha RMSE: 0.104; Beta RMSE: 0.091
[Epoch  5000] ELBO: 14971; Loglik: -13115; Acc.: 0.458; Alpha RMSE: 0.095; Beta RMSE: 0.074
[Epoch  5100] ELBO: 14842; Loglik: -12976; Acc.: 0.461; Alpha RMSE: 0.092; Beta RMSE: 0.081
[Epoch  5200] ELBO: 14916; Loglik: -13087; Acc.: 0.461; Alpha RMSE: 0.095; Beta RMSE: 0.070
[Epoch  5300] ELBO: 14887; Loglik: -13059; Acc.: 0.459; Alpha RMSE: 0.109; Beta RMSE: 0.076
[Epoch  5400] ELBO: 14920; Loglik: -13117; Acc.: 0.455; Alpha RMSE: 0.094; Beta RMSE: 0.062
[Epoch  5500] ELBO: 14921; Loglik: -13093; Acc.: 0.452; Alpha RMSE: 0.098; Beta RMSE: 0.062
[Epoch  5600] ELBO: 14876; Loglik: -13173; Acc.: 0.451; Alpha RMSE: 0.101; Beta RMSE: 0.069
[Epoch  5700] ELBO: 14884; Loglik: -13155; Acc.: 0.451; Alpha RMSE: 0.079; Beta RMSE: 0.074
[Epoch  5800] ELBO: 14883; Loglik: -13131; Acc.: 0.454; Alpha RMSE: 0.101; Beta RMSE: 0.067
[Epoch  5900] ELBO: 14907; Loglik: -13239; Acc.: 0.454; Alpha RMSE: 0.083; Beta RMSE: 0.063
[Epoch  6000] ELBO: 14886; Loglik: -13209; Acc.: 0.449; Alpha RMSE: 0.088; Beta RMSE: 0.064
[Epoch  6100] ELBO: 14897; Loglik: -13202; Acc.: 0.459; Alpha RMSE: 0.085; Beta RMSE: 0.057
[Epoch  6200] ELBO: 14775; Loglik: -13161; Acc.: 0.451; Alpha RMSE: 0.075; Beta RMSE: 0.061
[Epoch  6300] ELBO: 14811; Loglik: -13200; Acc.: 0.453; Alpha RMSE: 0.089; Beta RMSE: 0.058
[Epoch  6400] ELBO: 14890; Loglik: -13291; Acc.: 0.443; Alpha RMSE: 0.081; Beta RMSE: 0.050
[Epoch  6500] ELBO: 14804; Loglik: -13226; Acc.: 0.454; Alpha RMSE: 0.078; Beta RMSE: 0.060
[Epoch  6600] ELBO: 14912; Loglik: -13289; Acc.: 0.447; Alpha RMSE: 0.093; Beta RMSE: 0.042
[Epoch  6700] ELBO: 14749; Loglik: -13182; Acc.: 0.451; Alpha RMSE: 0.081; Beta RMSE: 0.040
[Epoch  6800] ELBO: 14824; Loglik: -13285; Acc.: 0.444; Alpha RMSE: 0.080; Beta RMSE: 0.051
[Epoch  6900] ELBO: 14835; Loglik: -13323; Acc.: 0.444; Alpha RMSE: 0.083; Beta RMSE: 0.041
[Epoch  7000] ELBO: 14910; Loglik: -13372; Acc.: 0.443; Alpha RMSE: 0.072; Beta RMSE: 0.038
[Epoch  7100] ELBO: 14906; Loglik: -13361; Acc.: 0.442; Alpha RMSE: 0.085; Beta RMSE: 0.042
[Epoch  7200] ELBO: 14867; Loglik: -13382; Acc.: 0.438; Alpha RMSE: 0.078; Beta RMSE: 0.031
[Epoch  7300] ELBO: 14705; Loglik: -13209; Acc.: 0.452; Alpha RMSE: 0.074; Beta RMSE: 0.043
[Epoch  7400] ELBO: 14862; Loglik: -13389; Acc.: 0.439; Alpha RMSE: 0.081; Beta RMSE: 0.033
[Epoch  7500] ELBO: 14812; Loglik: -13349; Acc.: 0.447; Alpha RMSE: 0.090; Beta RMSE: 0.036
[Epoch  7600] ELBO: 14754; Loglik: -13255; Acc.: 0.448; Alpha RMSE: 0.074; Beta RMSE: 0.034
[Epoch  7700] ELBO: 14827; Loglik: -13398; Acc.: 0.438; Alpha RMSE: 0.068; Beta RMSE: 0.039
[Epoch  7800] ELBO: 14874; Loglik: -13431; Acc.: 0.438; Alpha RMSE: 0.070; Beta RMSE: 0.032
[Epoch  7900] ELBO: 14815; Loglik: -13383; Acc.: 0.440; Alpha RMSE: 0.083; Beta RMSE: 0.033
[Epoch  8000] ELBO: 14843; Loglik: -13410; Acc.: 0.437; Alpha RMSE: 0.077; Beta RMSE: 0.033
[Epoch  8100] ELBO: 14827; Loglik: -13422; Acc.: 0.438; Alpha RMSE: 0.071; Beta RMSE: 0.030
[Epoch  8200] ELBO: 14830; Loglik: -13422; Acc.: 0.436; Alpha RMSE: 0.090; Beta RMSE: 0.028
[Epoch  8300] ELBO: 14794; Loglik: -13385; Acc.: 0.440; Alpha RMSE: 0.069; Beta RMSE: 0.031
[Epoch  8400] ELBO: 14797; Loglik: -13404; Acc.: 0.443; Alpha RMSE: 0.063; Beta RMSE: 0.024
[Epoch  8500] ELBO: 14901; Loglik: -13502; Acc.: 0.432; Alpha RMSE: 0.054; Beta RMSE: 0.032
[Epoch  8600] ELBO: 14889; Loglik: -13485; Acc.: 0.439; Alpha RMSE: 0.070; Beta RMSE: 0.023
[Epoch  8700] ELBO: 14873; Loglik: -13496; Acc.: 0.431; Alpha RMSE: 0.082; Beta RMSE: 0.023
[Epoch  8800] ELBO: 14794; Loglik: -13445; Acc.: 0.433; Alpha RMSE: 0.078; Beta RMSE: 0.021
[Epoch  8900] ELBO: 14805; Loglik: -13447; Acc.: 0.437; Alpha RMSE: 0.072; Beta RMSE: 0.025
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Epoch  9000] ELBO: 14775; Loglik: -13415; Acc.: 0.432; Alpha RMSE: 0.056; Beta RMSE: 0.022
[Epoch  9100] ELBO: 14774; Loglik: -13435; Acc.: 0.438; Alpha RMSE: 0.064; Beta RMSE: 0.021
[Epoch  9200] ELBO: 14752; Loglik: -13421; Acc.: 0.444; Alpha RMSE: 0.073; Beta RMSE: 0.023
[Epoch  9300] ELBO: 14797; Loglik: -13464; Acc.: 0.433; Alpha RMSE: 0.057; Beta RMSE: 0.019
[Epoch  9400] ELBO: 14849; Loglik: -13523; Acc.: 0.437; Alpha RMSE: 0.077; Beta RMSE: 0.020
[Epoch  9500] ELBO: 14794; Loglik: -13472; Acc.: 0.438; Alpha RMSE: 0.069; Beta RMSE: 0.020
[Epoch  9600] ELBO: 14820; Loglik: -13505; Acc.: 0.431; Alpha RMSE: 0.063; Beta RMSE: 0.015
[Epoch  9700] ELBO: 14803; Loglik: -13479; Acc.: 0.434; Alpha RMSE: 0.059; Beta RMSE: 0.013
[Epoch  9800] ELBO: 14823; Loglik: -13506; Acc.: 0.433; Alpha RMSE: 0.065; Beta RMSE: 0.018
[Epoch  9900] ELBO: 14842; Loglik: -13523; Acc.: 0.436; Alpha RMSE: 0.078; Beta RMSE: 0.014
[Epoch 10000] ELBO: 14758; Loglik: -13449; Acc.: 0.435; Alpha RMSE: 0.062; Beta RMSE: 0.021
[Epoch 10100] ELBO: 14728; Loglik: -13420; Acc.: 0.438; Alpha RMSE: 0.046; Beta RMSE: 0.012
[Epoch 10200] ELBO: 14710; Loglik: -13413; Acc.: 0.439; Alpha RMSE: 0.049; Beta RMSE: 0.021
[Epoch 10300] ELBO: 14775; Loglik: -13471; Acc.: 0.439; Alpha RMSE: 0.074; Beta RMSE: 0.012
[Epoch 10400] ELBO: 14796; Loglik: -13493; Acc.: 0.439; Alpha RMSE: 0.063; Beta RMSE: 0.020
[Epoch 10500] ELBO: 14744; Loglik: -13461; Acc.: 0.437; Alpha RMSE: 0.069; Beta RMSE: 0.019
[Epoch 10600] ELBO: 14775; Loglik: -13475; Acc.: 0.438; Alpha RMSE: 0.077; Beta RMSE: 0.018
[Epoch 10700] ELBO: 14806; Loglik: -13515; Acc.: 0.431; Alpha RMSE: 0.066; Beta RMSE: 0.022
[Epoch 10800] ELBO: 14827; Loglik: -13534; Acc.: 0.437; Alpha RMSE: 0.072; Beta RMSE: 0.017
[Epoch 10900] ELBO: 14763; Loglik: -13478; Acc.: 0.446; Alpha RMSE: 0.073; Beta RMSE: 0.014
[Epoch 11000] ELBO: 14781; Loglik: -13489; Acc.: 0.432; Alpha RMSE: 0.061; Beta RMSE: 0.014
[Epoch 11100] ELBO: 14752; Loglik: -13467; Acc.: 0.437; Alpha RMSE: 0.072; Beta RMSE: 0.013
[Epoch 11200] ELBO: 14878; Loglik: -13590; Acc.: 0.424; Alpha RMSE: 0.060; Beta RMSE: 0.015
[Epoch 11300] ELBO: 14820; Loglik: -13535; Acc.: 0.432; Alpha RMSE: 0.063; Beta RMSE: 0.021
[Epoch 11400] ELBO: 14842; Loglik: -13567; Acc.: 0.428; Alpha RMSE: 0.056; Beta RMSE: 0.016
[Epoch 11500] ELBO: 14856; Loglik: -13592; Acc.: 0.428; Alpha RMSE: 0.076; Beta RMSE: 0.015
[Epoch 11600] ELBO: 14789; Loglik: -13511; Acc.: 0.441; Alpha RMSE: 0.065; Beta RMSE: 0.018
[Epoch 11700] ELBO: 14773; Loglik: -13499; Acc.: 0.434; Alpha RMSE: 0.066; Beta RMSE: 0.023
[Epoch 11800] ELBO: 14789; Loglik: -13533; Acc.: 0.433; Alpha RMSE: 0.061; Beta RMSE: 0.013
[Epoch 11900] ELBO: 14751; Loglik: -13487; Acc.: 0.442; Alpha RMSE: 0.047; Beta RMSE: 0.019
[Epoch 12000] ELBO: 14792; Loglik: -13528; Acc.: 0.435; Alpha RMSE: 0.072; Beta RMSE: 0.019
[Epoch 12100] ELBO: 14791; Loglik: -13543; Acc.: 0.435; Alpha RMSE: 0.065; Beta RMSE: 0.016
[Epoch 12200] ELBO: 14760; Loglik: -13499; Acc.: 0.434; Alpha RMSE: 0.069; Beta RMSE: 0.015
[Epoch 12300] ELBO: 14765; Loglik: -13509; Acc.: 0.439; Alpha RMSE: 0.074; Beta RMSE: 0.016
[Epoch 12400] ELBO: 14833; Loglik: -13574; Acc.: 0.431; Alpha RMSE: 0.056; Beta RMSE: 0.015
[Epoch 12500] ELBO: 14883; Loglik: -13622; Acc.: 0.430; Alpha RMSE: 0.055; Beta RMSE: 0.024
[Epoch 12600] ELBO: 14780; Loglik: -13521; Acc.: 0.435; Alpha RMSE: 0.068; Beta RMSE: 0.014
[Epoch 12700] ELBO: 14775; Loglik: -13518; Acc.: 0.433; Alpha RMSE: 0.067; Beta RMSE: 0.015
[Epoch 12800] ELBO: 14854; Loglik: -13600; Acc.: 0.434; Alpha RMSE: 0.070; Beta RMSE: 0.021
[Epoch 12900] ELBO: 14818; Loglik: -13567; Acc.: 0.433; Alpha RMSE: 0.058; Beta RMSE: 0.018
[Epoch 13000] ELBO: 14772; Loglik: -13503; Acc.: 0.433; Alpha RMSE: 0.068; Beta RMSE: 0.020
[Epoch 13100] ELBO: 14829; Loglik: -13588; Acc.: 0.426; Alpha RMSE: 0.066; Beta RMSE: 0.015
[Epoch 13200] ELBO: 14753; Loglik: -13493; Acc.: 0.434; Alpha RMSE: 0.075; Beta RMSE: 0.016
[Epoch 13300] ELBO: 14876; Loglik: -13633; Acc.: 0.424; Alpha RMSE: 0.074; Beta RMSE: 0.014
[Epoch 13400] ELBO: 14802; Loglik: -13541; Acc.: 0.437; Alpha RMSE: 0.054; Beta RMSE: 0.015
[Epoch 13500] ELBO: 14763; Loglik: -13511; Acc.: 0.432; Alpha RMSE: 0.067; Beta RMSE: 0.015
[Epoch 13600] ELBO: 14861; Loglik: -13609; Acc.: 0.430; Alpha RMSE: 0.063; Beta RMSE: 0.012
[Epoch 13700] ELBO: 14793; Loglik: -13551; Acc.: 0.438; Alpha RMSE: 0.059; Beta RMSE: 0.014
[Epoch 13800] ELBO: 14752; Loglik: -13513; Acc.: 0.435; Alpha RMSE: 0.059; Beta RMSE: 0.010
[Epoch 13900] ELBO: 14813; Loglik: -13567; Acc.: 0.432; Alpha RMSE: 0.072; Beta RMSE: 0.020
[Epoch 14000] ELBO: 14793; Loglik: -13551; Acc.: 0.427; Alpha RMSE: 0.063; Beta RMSE: 0.014
[Epoch 14100] ELBO: 14883; Loglik: -13637; Acc.: 0.429; Alpha RMSE: 0.079; Beta RMSE: 0.014
[Epoch 14200] ELBO: 14779; Loglik: -13530; Acc.: 0.433; Alpha RMSE: 0.056; Beta RMSE: 0.019
[Epoch 14300] ELBO: 14774; Loglik: -13518; Acc.: 0.428; Alpha RMSE: 0.061; Beta RMSE: 0.023
[Epoch 14400] ELBO: 14738; Loglik: -13488; Acc.: 0.432; Alpha RMSE: 0.051; Beta RMSE: 0.016
[Epoch 14500] ELBO: 14791; Loglik: -13541; Acc.: 0.431; Alpha RMSE: 0.054; Beta RMSE: 0.014
[Epoch 14600] ELBO: 14829; Loglik: -13594; Acc.: 0.427; Alpha RMSE: 0.077; Beta RMSE: 0.011
[Epoch 14700] ELBO: 14836; Loglik: -13612; Acc.: 0.428; Alpha RMSE: 0.071; Beta RMSE: 0.015
[Epoch 14800] ELBO: 14809; Loglik: -13572; Acc.: 0.431; Alpha RMSE: 0.061; Beta RMSE: 0.016
[Epoch 14900] ELBO: 14761; Loglik: -13528; Acc.: 0.428; Alpha RMSE: 0.057; Beta RMSE: 0.012
[Epoch 15000] ELBO: 14771; Loglik: -13530; Acc.: 0.428; Alpha RMSE: 0.052; Beta RMSE: 0.017
[Epoch 15100] ELBO: 14793; Loglik: -13553; Acc.: 0.431; Alpha RMSE: 0.061; Beta RMSE: 0.013
[Epoch 15200] ELBO: 14826; Loglik: -13580; Acc.: 0.429; Alpha RMSE: 0.070; Beta RMSE: 0.019
[Epoch 15300] ELBO: 14753; Loglik: -13506; Acc.: 0.431; Alpha RMSE: 0.052; Beta RMSE: 0.017
[Epoch 15400] ELBO: 14805; Loglik: -13561; Acc.: 0.434; Alpha RMSE: 0.049; Beta RMSE: 0.013
[Epoch 15500] ELBO: 14804; Loglik: -13562; Acc.: 0.428; Alpha RMSE: 0.054; Beta RMSE: 0.012
[Epoch 15600] ELBO: 14847; Loglik: -13617; Acc.: 0.428; Alpha RMSE: 0.074; Beta RMSE: 0.014
[Epoch 15700] ELBO: 14851; Loglik: -13623; Acc.: 0.433; Alpha RMSE: 0.065; Beta RMSE: 0.011
[Epoch 15800] ELBO: 14829; Loglik: -13588; Acc.: 0.427; Alpha RMSE: 0.065; Beta RMSE: 0.016
[Epoch 15900] ELBO: 14788; Loglik: -13553; Acc.: 0.429; Alpha RMSE: 0.062; Beta RMSE: 0.012
[Epoch 16000] ELBO: 14825; Loglik: -13595; Acc.: 0.427; Alpha RMSE: 0.052; Beta RMSE: 0.012
[Epoch 16100] ELBO: 14703; Loglik: -13474; Acc.: 0.438; Alpha RMSE: 0.051; Beta RMSE: 0.016
[Epoch 16200] ELBO: 14768; Loglik: -13524; Acc.: 0.433; Alpha RMSE: 0.055; Beta RMSE: 0.015
[Epoch 16300] ELBO: 14757; Loglik: -13520; Acc.: 0.431; Alpha RMSE: 0.070; Beta RMSE: 0.012
[Epoch 16400] ELBO: 14798; Loglik: -13561; Acc.: 0.434; Alpha RMSE: 0.063; Beta RMSE: 0.013
[Epoch 16500] ELBO: 14751; Loglik: -13510; Acc.: 0.429; Alpha RMSE: 0.057; Beta RMSE: 0.009
[Epoch 16600] ELBO: 14805; Loglik: -13566; Acc.: 0.434; Alpha RMSE: 0.052; Beta RMSE: 0.011
[Epoch 16700] ELBO: 14821; Loglik: -13582; Acc.: 0.426; Alpha RMSE: 0.066; Beta RMSE: 0.011
[Epoch 16800] ELBO: 14826; Loglik: -13601; Acc.: 0.427; Alpha RMSE: 0.052; Beta RMSE: 0.013
[Epoch 16900] ELBO: 14777; Loglik: -13553; Acc.: 0.429; Alpha RMSE: 0.057; Beta RMSE: 0.008
[Epoch 17000] ELBO: 14770; Loglik: -13536; Acc.: 0.437; Alpha RMSE: 0.065; Beta RMSE: 0.016
[Epoch 17100] ELBO: 14800; Loglik: -13570; Acc.: 0.431; Alpha RMSE: 0.057; Beta RMSE: 0.011
[Epoch 17200] ELBO: 14822; Loglik: -13589; Acc.: 0.435; Alpha RMSE: 0.065; Beta RMSE: 0.012
[Epoch 17300] ELBO: 14799; Loglik: -13565; Acc.: 0.430; Alpha RMSE: 0.063; Beta RMSE: 0.016
[Epoch 17400] ELBO: 14832; Loglik: -13598; Acc.: 0.429; Alpha RMSE: 0.053; Beta RMSE: 0.014
[Epoch 17500] ELBO: 14785; Loglik: -13550; Acc.: 0.432; Alpha RMSE: 0.074; Beta RMSE: 0.017
[Epoch 17600] ELBO: 14815; Loglik: -13575; Acc.: 0.428; Alpha RMSE: 0.050; Beta RMSE: 0.014
[Epoch 17700] ELBO: 14831; Loglik: -13591; Acc.: 0.436; Alpha RMSE: 0.059; Beta RMSE: 0.019
[Epoch 17800] ELBO: 14801; Loglik: -13564; Acc.: 0.433; Alpha RMSE: 0.064; Beta RMSE: 0.015
[Epoch 17900] ELBO: 14805; Loglik: -13584; Acc.: 0.436; Alpha RMSE: 0.075; Beta RMSE: 0.013
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Epoch 18000] ELBO: 14697; Loglik: -13468; Acc.: 0.439; Alpha RMSE: 0.051; Beta RMSE: 0.012
[Epoch 18100] ELBO: 14773; Loglik: -13553; Acc.: 0.431; Alpha RMSE: 0.067; Beta RMSE: 0.010
[Epoch 18200] ELBO: 14837; Loglik: -13610; Acc.: 0.422; Alpha RMSE: 0.048; Beta RMSE: 0.014
[Epoch 18300] ELBO: 14815; Loglik: -13585; Acc.: 0.437; Alpha RMSE: 0.072; Beta RMSE: 0.015
[Epoch 18400] ELBO: 14792; Loglik: -13555; Acc.: 0.427; Alpha RMSE: 0.060; Beta RMSE: 0.017
[Epoch 18500] ELBO: 14788; Loglik: -13562; Acc.: 0.434; Alpha RMSE: 0.067; Beta RMSE: 0.020
[Epoch 18600] ELBO: 14728; Loglik: -13493; Acc.: 0.434; Alpha RMSE: 0.059; Beta RMSE: 0.015
[Epoch 18700] ELBO: 14827; Loglik: -13605; Acc.: 0.429; Alpha RMSE: 0.053; Beta RMSE: 0.013
[Epoch 18800] ELBO: 14785; Loglik: -13549; Acc.: 0.432; Alpha RMSE: 0.068; Beta RMSE: 0.012
[Epoch 18900] ELBO: 14805; Loglik: -13567; Acc.: 0.428; Alpha RMSE: 0.056; Beta RMSE: 0.007
[Epoch 19000] ELBO: 14793; Loglik: -13559; Acc.: 0.433; Alpha RMSE: 0.060; Beta RMSE: 0.008
[Epoch 19100] ELBO: 14833; Loglik: -13595; Acc.: 0.431; Alpha RMSE: 0.069; Beta RMSE: 0.014
[Epoch 19200] ELBO: 14822; Loglik: -13593; Acc.: 0.429; Alpha RMSE: 0.063; Beta RMSE: 0.014
[Epoch 19300] ELBO: 14805; Loglik: -13572; Acc.: 0.431; Alpha RMSE: 0.048; Beta RMSE: 0.015
[Epoch 19400] ELBO: 14766; Loglik: -13549; Acc.: 0.431; Alpha RMSE: 0.069; Beta RMSE: 0.017
[Epoch 19500] ELBO: 14783; Loglik: -13545; Acc.: 0.433; Alpha RMSE: 0.060; Beta RMSE: 0.009
[Epoch 19600] ELBO: 14766; Loglik: -13537; Acc.: 0.434; Alpha RMSE: 0.073; Beta RMSE: 0.013
[Epoch 19700] ELBO: 14747; Loglik: -13520; Acc.: 0.433; Alpha RMSE: 0.042; Beta RMSE: 0.012
[Epoch 19800] ELBO: 14812; Loglik: -13583; Acc.: 0.430; Alpha RMSE: 0.057; Beta RMSE: 0.013
[Epoch 19900] ELBO: 14804; Loglik: -13581; Acc.: 0.427; Alpha RMSE: 0.089; Beta RMSE: 0.013
Elapsed time: 304.10623359680176 

True alpha: [-0.8  0.8  0. ]
Est. alpha: [-8.2980311e-01  8.8909477e-01  5.0747342e-04]
	BETA_XF1: -0.830
	BETA_XF2: 0.889
	BETA_XF3: 0.001

True zeta: [-0.8  0.8  1.  -0.8  0. ]
Est. zeta: [-0.817338    0.7997669   0.9919458  -0.81087536 -0.00533627]
	BETA_XR1: -0.817
	BETA_XR2: 0.800
	BETA_XR3: 0.992
	BETA_XR4: -0.811
	BETA_XR5: -0.005
</pre></div>
</div>
<img alt="../_images/mxl_ard_13_3.png" src="../_images/mxl_ard_13_3.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 53min, sys: 2.58 s, total: 53min 3s
Wall time: 5min 7s
</pre></div>
</div>
</div>
</div>
<p>Lets now have a look at the inferred prior variances. Irrelevant components in the utility will have the expected values of the prior variances of their corresponding parameters shrunk to values close to zero. Starting with the expected values of the prior variances for the fixed effects:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">mxl</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">mxl</span><span class="o">.</span><span class="n">tau_alpha_alpha</span><span class="p">)</span> <span class="o">/</span> <span class="n">mxl</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">mxl</span><span class="o">.</span><span class="n">tau_alpha_beta</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([1.0889, 1.0683, 0.0384], device=&#39;cuda:0&#39;, grad_fn=&lt;MulBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>We can see that the last preference parameter has a value close to zero, which indicates that the inclusion of that term in the utility function is irrelevant. This is the desired behaviour given the way that the data was generated.</p>
<p>Lets now have a look at the expected values of the prior variances for the random effects:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">mxl</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">mxl</span><span class="o">.</span><span class="n">tau_zeta_alpha</span><span class="p">)</span> <span class="o">/</span> <span class="n">mxl</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">mxl</span><span class="o">.</span><span class="n">tau_zeta_beta</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([1.0584, 1.0380, 1.3097, 1.0221, 0.0225], device=&#39;cuda:0&#39;,
       grad_fn=&lt;MulBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>As before, can see that the last preference parameter has a value close to zero, which indicates that the inclusion of that term in the utility function is irrelevant. This is the desired behaviour given the way that the data was generated.</p>
<p>The “results” dictionary containts a summary of the results of variational inference, including means of the posterior approximations for the different parameters in the model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;Estimation time&#39;: 304.10623359680176,
 &#39;Est. alpha&#39;: array([-8.2980311e-01,  8.8909477e-01,  5.0747342e-04], dtype=float32),
 &#39;Est. zeta&#39;: array([-0.817338  ,  0.7997669 ,  0.9919458 , -0.81087536, -0.00533627],
       dtype=float32),
 &#39;Est. beta_n&#39;: array([[-1.238147  ,  0.35534793,  0.56024605, -0.9264143 , -0.441626  ],
        [-0.5745481 ,  1.3353534 ,  1.3706696 , -0.3979419 ,  0.5681873 ],
        [-1.7197372 , -0.48435944,  0.08551951, -1.7841139 , -0.9252483 ],
        ...,
        [ 0.24848358,  2.0946019 ,  2.317096  ,  0.45183292,  1.5201857 ],
        [-0.81808007,  0.84409773,  1.0342833 , -0.71131504, -0.04303875],
        [-0.6825954 ,  1.1738247 ,  1.2203168 , -0.5835304 ,  0.14842193]],
       dtype=float32),
 &#39;ELBO&#39;: 14744.6484375,
 &#39;Loglikelihood&#39;: -13506.572265625,
 &#39;Accuracy&#39;: 0.43320000171661377}
</pre></div>
</div>
</div>
</div>
<p>This interface is currently being improved to include additional output information, but additional information can be obtained from the attributes of the “mxl” object for now.</p>
</section>
<section id="implementation-details">
<h2>Implementation details<a class="headerlink" href="#implementation-details" title="Permalink to this headline">¶</a></h2>
<p>Modifying the core MXL model implementation with this ARD extension is relatively straightforward. The generative process described above is different from the core MXL model, so we begin by introducing the two new priors (<span class="math notranslate nohighlight">\(\boldsymbol\tau_\alpha\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol\tau_\zeta\)</span>) in the <code class="docutils literal notranslate"><span class="pre">elbo()</span></code> function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tau_alpha_prior</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">Gamma</span><span class="p">(</span><span class="mf">.01</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_fixed_params</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> 
                                   <span class="mf">.01</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_fixed_params</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>

<span class="n">tau_zeta_prior</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">Gamma</span><span class="p">(</span><span class="mf">.01</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_mixed_params</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> 
                          <span class="mf">.01</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_mixed_params</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
</pre></div>
</div>
<p>Note that we use the Gamma distribution, but we will later take the inverse of its samples.</p>
<p>Now we must also change the priors over <span class="math notranslate nohighlight">\(\boldsymbol\alpha\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol\zeta\)</span> to use the newly defined priors over the variances:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">alpha_prior</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_fixed_params</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> 
                        <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">/</span><span class="n">tau_alpha</span><span class="p">)</span>

<span class="n">zeta_prior</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_mixed_params</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
                               <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">/</span><span class="n">tau_zeta</span><span class="p">)</span>
</pre></div>
</div>
<p>Our goal is to use Variational Inference to compute (approximate) posterior distributions over <span class="math notranslate nohighlight">\(\boldsymbol\tau_\alpha\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol\tau_\zeta\)</span>. Therefore, we must introduce two new approximate distributions in the <code class="docutils literal notranslate"><span class="pre">compute_variational_approximation_q()</span></code> method: <span class="math notranslate nohighlight">\(q(\boldsymbol\tau_\alpha)\)</span> and <span class="math notranslate nohighlight">\(q(\boldsymbol\tau_\zeta)\)</span>. Will use Gamma distributions as the approximate distribution for <span class="math notranslate nohighlight">\(\boldsymbol\tau_\alpha\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol\tau_\zeta\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># q(tau_alpha) - construct posterior approximation on tau_alpha</span>
<span class="n">q_tau_alpha</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">Gamma</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tau_alpha_alpha</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tau_alpha_beta</span><span class="p">))</span>

<span class="c1"># q(tau_zeta) - construct posterior approximation on tau_zeta</span>
<span class="n">q_tau_zeta</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">Gamma</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tau_zeta_alpha</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tau_zeta_beta</span><span class="p">))</span>
</pre></div>
</div>
<p>Note that these 2 new variational distributions have learnable parameters (we will later optimize the ELBO w.r.t. their values) which we will have to initialize in the <code class="docutils literal notranslate"><span class="pre">initialize_variational_distribution_q()</span></code> function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># q(tau_alpha) - initialize parameters of InverseGamma approximation</span>
<span class="bp">self</span><span class="o">.</span><span class="n">tau_alpha_alpha</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_fixed_params</span><span class="p">))</span>
<span class="bp">self</span><span class="o">.</span><span class="n">tau_alpha_beta</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_fixed_params</span><span class="p">))</span>

<span class="c1"># q(tau_zeta) - initialize parameters of InverseGamma approximation</span>
<span class="bp">self</span><span class="o">.</span><span class="n">tau_zeta_alpha</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_mixed_params</span><span class="p">))</span>
<span class="bp">self</span><span class="o">.</span><span class="n">tau_zeta_beta</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_mixed_params</span><span class="p">))</span>
</pre></div>
</div>
<p>At this point, we are almost done. We are just missing updating the ELBO, which now has to include two additional KL terms that result from the two new priors in the generative process: <span class="math notranslate nohighlight">\(\mbox{KL}[q(\boldsymbol\tau_\alpha) || p(\boldsymbol\tau_\alpha)]\)</span> and <span class="math notranslate nohighlight">\(\mbox{KL}[q(\boldsymbol\tau_\zeta) || p(\boldsymbol\tau_\zeta)]\)</span>. We can add these 2 extra terms to the ELBO by including the following 2 lines in the ELBO computation done in by the <code class="docutils literal notranslate"><span class="pre">elbo()</span></code> function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># KL[q(tau_alpha) || p(tau_alpha)]</span>
<span class="n">kld</span> <span class="o">+=</span> <span class="n">td</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">q_tau_alpha</span><span class="p">,</span> <span class="n">tau_alpha_prior</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># KL[q(tau_zeta) || p(tau_zeta)]</span>
<span class="n">kld</span> <span class="o">+=</span> <span class="n">td</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">q_tau_zeta</span><span class="p">,</span> <span class="n">tau_zeta_prior</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
<p>And we are done! We have implemented ARD in the core MXL model. After convergence of the ELBO, we can inspect the values of the variational parameters <code class="docutils literal notranslate"><span class="pre">self.tau_alpha_alpha</span></code>, <code class="docutils literal notranslate"><span class="pre">self.tau_alpha_beta</span></code>, <code class="docutils literal notranslate"><span class="pre">self.tau_zeta_alpha</span></code> and <code class="docutils literal notranslate"><span class="pre">self.tau_zeta_beta</span></code> for the results of the ARD selection process as we have done above in this notebook.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./extensions"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../extensions.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Extensions</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="nnet_utility.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Neural networks in utilities</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Author<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>