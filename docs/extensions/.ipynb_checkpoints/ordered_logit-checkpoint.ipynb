{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed ordered logit\n",
    "\n",
    "We begin by performing the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/rodr/code/amortized-mxl-dev/release\") \n",
    "\n",
    "import logging\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate simulated data\n",
    "\n",
    "We predefine the fixed effects parameters (true_alpha) and random effects parameters (true_beta), as well as the covariance matrix (true_Omega), and sample simulated choice data for 500 respondents (num_resp), each with 5 choice situations (num_menus). The number of choice alternatives is set to 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>choice</th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>indID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.585995</td>\n",
       "      <td>0.667338</td>\n",
       "      <td>0.681352</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.476451</td>\n",
       "      <td>0.026819</td>\n",
       "      <td>0.443944</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.753613</td>\n",
       "      <td>0.768395</td>\n",
       "      <td>0.468803</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.605222</td>\n",
       "      <td>0.862393</td>\n",
       "      <td>0.434364</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142990</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.608124</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   choice   x0        x1        x2        x3  indID\n",
       "0       0  1.0  0.585995  0.667338  0.681352      0\n",
       "1       0  1.0  0.476451  0.026819  0.443944      1\n",
       "2       0  1.0  0.753613  0.768395  0.468803      2\n",
       "3       2  1.0  0.605222  0.862393  0.434364      3\n",
       "4       0  1.0  0.142990  0.001069  0.608124      4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/fake_data_ordered.csv', index_col=0)\n",
    "num_resp = len(df)\n",
    "df['indID'] = np.arange(num_resp)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Logit specification\n",
    "\n",
    "We now make use of the developed formula interface to specify the utilities of the mixed logit model. \n",
    "\n",
    "We begin by defining the fixed effects parameters, the random effects parameters, and the observed variables. This creates instances of Python objects that can be put together to define the utility functions for the different alternatives.\n",
    "\n",
    "Once the utilities are defined, we collect them in a Python dictionary mapping alternative names to their corresponding expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.dcm_interface import FixedEffect, RandomEffect, ObservedVariable\n",
    "import torch.distributions as dists\n",
    "\n",
    "# define fixed effects parameters\n",
    "B_X0 = FixedEffect('BETA_X0')\n",
    "B_X1 = FixedEffect('BETA_X1')\n",
    "\n",
    "# define random effects parameters\n",
    "B_X2 = RandomEffect('BETA_X2')\n",
    "B_X3 = RandomEffect('BETA_X3')\n",
    "\n",
    "# define observed variables\n",
    "for attr in df.columns:\n",
    "    exec(\"%s = ObservedVariable('%s')\" % (attr,attr))\n",
    "\n",
    "# define utility functions\n",
    "V1 = B_X0*x0 + B_X1*x1 + B_X2*x2 + B_X3*x3\n",
    "\n",
    "# associate utility functions with the names of the alternatives\n",
    "utilities = {\"ALT1\": V1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to create a Specification object containing the utilities that we have just defined. Note that we must also specify the type of choice model to be used - a mixed logit model (MXL) in this case.\n",
    "\n",
    "Note that we can inspect the specification by printing the dcm_spec object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- MXL specification:\n",
      "Alternatives: ['ALT1']\n",
      "Utility functions:\n",
      "   V_ALT1 = BETA_X0*x0 + BETA_X1*x1 + BETA_X2_n*x2 + BETA_X3_n*x3\n",
      "\n",
      "Num. parameters to be estimated: 4\n",
      "Fixed effects params: ['BETA_X0', 'BETA_X1']\n",
      "Random effects params: ['BETA_X2', 'BETA_X3']\n"
     ]
    }
   ],
   "source": [
    "from core.dcm_interface import Specification\n",
    "\n",
    "#Logit(choice, utilities, availability, df)\n",
    "#Logit(choice_test, utilities, availability_test, df_test)\n",
    "\n",
    "# create MXL specification object based on the utilities previously defined\n",
    "dcm_spec = Specification('MXL', utilities)\n",
    "print(dcm_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Specification is defined, we need to define the DCM Dataset object that goes along with it. For this, we instantiate the Dataset class with the Pandas dataframe containing the data in the so-called \"wide format\", the name of column in the dataframe containing the observed choices and the dcm_spec that we have previously created.\n",
    "\n",
    "Note that since this is panel data, we must also specify the name of the column in the dataframe that contains the ID of the respondent (this should be a integer ranging from 0 the num_resp-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "\tModel type: MXL\n",
      "\tNum. observations: 10000\n",
      "\tNum. alternatives: 1\n",
      "\tNum. respondents: 10000\n",
      "\tNum. menus: 1\n",
      "\tObservations IDs: [   0    1    2 ... 9997 9998 9999]\n",
      "\tAlternative IDs: None\n",
      "\tRespondent IDs: [   0    1    2 ... 9997 9998 9999]\n",
      "\tAvailability columns: None\n",
      "\tAttribute names: ['x0', 'x1', 'x2', 'x3']\n",
      "\tFixed effects attribute names: ['x0', 'x1']\n",
      "\tFixed effects parameter names: ['BETA_X0', 'BETA_X1']\n",
      "\tRandom effects attribute names: ['x2', 'x3']\n",
      "\tRandom effects parameter names: ['BETA_X2', 'BETA_X3']\n",
      "\tAlternative attributes ndarray.shape: (10000, 1, 4)\n",
      "\tChoices ndarray.shape: (10000, 1)\n",
      "\tAlternatives availability ndarray.shape: (10000, 1, 1)\n",
      "\tData mask ndarray.shape: (10000, 1)\n",
      "\tContext data ndarray.shape: (10000, 0)\n",
      "\tNeural nets data ndarray.shape: (10000, 0)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from core.dcm_interface import Dataset\n",
    "\n",
    "# create DCM dataset object\n",
    "dcm_dataset = Dataset(df, 'choice', dcm_spec, resp_id_col='indID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the specification, we can inspect the DCM dataset by printing the dcm_dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- DCM dataset:\n",
      "Model type: MXL\n",
      "Num. observations: 10000\n",
      "Num. alternatives: 1\n",
      "Num. respondents: 10000\n",
      "Num. menus: 1\n",
      "Num. fixed effects: 2\n",
      "Num. random effects: 2\n",
      "Attribute names: ['x0', 'x1', 'x2', 'x3']\n"
     ]
    }
   ],
   "source": [
    "print(dcm_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Mixed Logit Model in PyTorch\n",
    "\n",
    "It is now time to perform approximate Bayesian inference on the mixed logit model that we have specified. The generative process of the MXL model that we will be using is the following:\n",
    "\n",
    "1. Draw fixed taste parameters $\\boldsymbol\\alpha \\sim \\mathcal{N}(\\boldsymbol\\lambda_0, \\boldsymbol\\Xi_0)$\n",
    "2. Draw mean vector $\\boldsymbol\\zeta \\sim \\mathcal{N}(\\boldsymbol\\mu_0, \\boldsymbol\\Sigma_0)$\n",
    "3. Draw scales vector $\\boldsymbol\\theta \\sim \\mbox{half-Cauchy}(\\boldsymbol\\sigma_0)$\n",
    "4. Draw correlation matrix $\\boldsymbol\\Psi \\sim \\mbox{LKJ}(\\nu)$\n",
    "5. For each decision-maker $n \\in \\{1,\\dots,N\\}$\n",
    "    1. Draw random taste parameters $\\boldsymbol\\beta_n \\sim \\mathcal{N}(\\boldsymbol\\zeta,\\boldsymbol\\Omega)$\n",
    "    2. For each choice occasion $t \\in \\{1,\\dots,T_n\\}$\n",
    "        1. Draw observed choice $y_{nt} \\sim \\mbox{MNL}(\\boldsymbol\\alpha, \\boldsymbol\\beta_n, \\textbf{X}_{nt})$\n",
    "        \n",
    "where $\\boldsymbol\\Omega = \\mbox{diag}(\\boldsymbol\\theta) \\times \\boldsymbol\\Psi \\times  \\mbox{diag}(\\boldsymbol\\theta)$.\n",
    "\n",
    "We can instantiate this model from the TorchMXL using the following code. We can the run variational inference to approximate the posterior distribution of the latent variables in the model. Note that since in this case we know the true parameters that were used to generate the simualated choice data, we can pass them to the \"infer\" method in order to obtain additional information during the ELBO maximization (useful for tracking the progress of VI and for other debugging purposes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch     0] ELBO: 27499; Loglik: -19145; Acc.: 0.388; Alpha RMSE: 1.000; Beta RMSE: 1.000\n",
      "[Epoch   100] ELBO: 21194; Loglik: -16886; Acc.: 0.388; Alpha RMSE: 0.919; Beta RMSE: 0.977\n",
      "[Epoch   200] ELBO: 16977; Loglik: -14373; Acc.: 0.388; Alpha RMSE: 0.861; Beta RMSE: 1.033\n",
      "[Epoch   300] ELBO: 17310; Loglik: -14032; Acc.: 0.388; Alpha RMSE: 0.781; Beta RMSE: 0.973\n",
      "[Epoch   400] ELBO: 16730; Loglik: -13663; Acc.: 0.388; Alpha RMSE: 0.694; Beta RMSE: 0.989\n",
      "[Epoch   500] ELBO: 18470; Loglik: -14556; Acc.: 0.388; Alpha RMSE: 0.615; Beta RMSE: 0.957\n",
      "[Epoch   600] ELBO: 17107; Loglik: -13061; Acc.: 0.388; Alpha RMSE: 0.556; Beta RMSE: 0.902\n",
      "[Epoch   700] ELBO: 15702; Loglik: -13136; Acc.: 0.388; Alpha RMSE: 0.454; Beta RMSE: 0.857\n",
      "[Epoch   800] ELBO: 15905; Loglik: -12963; Acc.: 0.388; Alpha RMSE: 0.411; Beta RMSE: 0.854\n",
      "[Epoch   900] ELBO: 16366; Loglik: -13187; Acc.: 0.388; Alpha RMSE: 0.354; Beta RMSE: 0.841\n",
      "[Epoch  1000] ELBO: 15607; Loglik: -12871; Acc.: 0.388; Alpha RMSE: 0.283; Beta RMSE: 0.828\n",
      "[Epoch  1100] ELBO: 15537; Loglik: -13055; Acc.: 0.388; Alpha RMSE: 0.200; Beta RMSE: 0.797\n",
      "[Epoch  1200] ELBO: 15407; Loglik: -12926; Acc.: 0.388; Alpha RMSE: 0.152; Beta RMSE: 0.767\n",
      "[Epoch  1300] ELBO: 15423; Loglik: -13005; Acc.: 0.388; Alpha RMSE: 0.114; Beta RMSE: 0.750\n",
      "[Epoch  1400] ELBO: 15257; Loglik: -12870; Acc.: 0.388; Alpha RMSE: 0.083; Beta RMSE: 0.703\n",
      "[Epoch  1500] ELBO: 15190; Loglik: -12845; Acc.: 0.388; Alpha RMSE: 0.083; Beta RMSE: 0.668\n",
      "[Epoch  1600] ELBO: 15170; Loglik: -12951; Acc.: 0.388; Alpha RMSE: 0.062; Beta RMSE: 0.624\n",
      "[Epoch  1700] ELBO: 15020; Loglik: -12974; Acc.: 0.388; Alpha RMSE: 0.080; Beta RMSE: 0.621\n",
      "[Epoch  1800] ELBO: 15061; Loglik: -12967; Acc.: 0.388; Alpha RMSE: 0.093; Beta RMSE: 0.580\n",
      "[Epoch  1900] ELBO: 14991; Loglik: -12960; Acc.: 0.388; Alpha RMSE: 0.095; Beta RMSE: 0.561\n",
      "[Epoch  2000] ELBO: 15001; Loglik: -12960; Acc.: 0.388; Alpha RMSE: 0.095; Beta RMSE: 0.525\n",
      "[Epoch  2100] ELBO: 15065; Loglik: -13086; Acc.: 0.388; Alpha RMSE: 0.090; Beta RMSE: 0.480\n",
      "[Epoch  2200] ELBO: 14955; Loglik: -13086; Acc.: 0.388; Alpha RMSE: 0.097; Beta RMSE: 0.453\n",
      "[Epoch  2300] ELBO: 14764; Loglik: -13034; Acc.: 0.388; Alpha RMSE: 0.109; Beta RMSE: 0.412\n",
      "[Epoch  2400] ELBO: 14825; Loglik: -13155; Acc.: 0.388; Alpha RMSE: 0.108; Beta RMSE: 0.377\n",
      "[Epoch  2500] ELBO: 14645; Loglik: -13068; Acc.: 0.388; Alpha RMSE: 0.103; Beta RMSE: 0.354\n",
      "[Epoch  2600] ELBO: 14688; Loglik: -13143; Acc.: 0.388; Alpha RMSE: 0.101; Beta RMSE: 0.320\n",
      "[Epoch  2700] ELBO: 14792; Loglik: -13292; Acc.: 0.388; Alpha RMSE: 0.089; Beta RMSE: 0.287\n",
      "[Epoch  2800] ELBO: 14751; Loglik: -13205; Acc.: 0.388; Alpha RMSE: 0.087; Beta RMSE: 0.255\n",
      "[Epoch  2900] ELBO: 14741; Loglik: -13362; Acc.: 0.388; Alpha RMSE: 0.091; Beta RMSE: 0.234\n",
      "[Epoch  3000] ELBO: 14721; Loglik: -13307; Acc.: 0.388; Alpha RMSE: 0.084; Beta RMSE: 0.196\n",
      "[Epoch  3100] ELBO: 14654; Loglik: -13252; Acc.: 0.388; Alpha RMSE: 0.080; Beta RMSE: 0.164\n",
      "[Epoch  3200] ELBO: 14737; Loglik: -13477; Acc.: 0.388; Alpha RMSE: 0.087; Beta RMSE: 0.141\n",
      "[Epoch  3300] ELBO: 14692; Loglik: -13421; Acc.: 0.388; Alpha RMSE: 0.067; Beta RMSE: 0.109\n",
      "[Epoch  3400] ELBO: 14623; Loglik: -13422; Acc.: 0.388; Alpha RMSE: 0.069; Beta RMSE: 0.103\n",
      "[Epoch  3500] ELBO: 14714; Loglik: -13563; Acc.: 0.388; Alpha RMSE: 0.062; Beta RMSE: 0.077\n",
      "[Epoch  3600] ELBO: 14673; Loglik: -13583; Acc.: 0.388; Alpha RMSE: 0.074; Beta RMSE: 0.049\n",
      "[Epoch  3700] ELBO: 14607; Loglik: -13489; Acc.: 0.388; Alpha RMSE: 0.068; Beta RMSE: 0.045\n",
      "[Epoch  3800] ELBO: 14535; Loglik: -13459; Acc.: 0.388; Alpha RMSE: 0.064; Beta RMSE: 0.046\n",
      "[Epoch  3900] ELBO: 14583; Loglik: -13551; Acc.: 0.388; Alpha RMSE: 0.051; Beta RMSE: 0.057\n",
      "[Epoch  4000] ELBO: 14540; Loglik: -13544; Acc.: 0.388; Alpha RMSE: 0.054; Beta RMSE: 0.064\n",
      "[Epoch  4100] ELBO: 14478; Loglik: -13536; Acc.: 0.388; Alpha RMSE: 0.065; Beta RMSE: 0.059\n",
      "[Epoch  4200] ELBO: 14642; Loglik: -13678; Acc.: 0.388; Alpha RMSE: 0.068; Beta RMSE: 0.086\n",
      "[Epoch  4300] ELBO: 14467; Loglik: -13585; Acc.: 0.388; Alpha RMSE: 0.053; Beta RMSE: 0.083\n",
      "[Epoch  4400] ELBO: 14575; Loglik: -13664; Acc.: 0.388; Alpha RMSE: 0.059; Beta RMSE: 0.083\n",
      "[Epoch  4500] ELBO: 14594; Loglik: -13703; Acc.: 0.388; Alpha RMSE: 0.048; Beta RMSE: 0.118\n",
      "[Epoch  4600] ELBO: 14581; Loglik: -13752; Acc.: 0.388; Alpha RMSE: 0.052; Beta RMSE: 0.115\n",
      "[Epoch  4700] ELBO: 14455; Loglik: -13658; Acc.: 0.388; Alpha RMSE: 0.063; Beta RMSE: 0.122\n",
      "[Epoch  4800] ELBO: 14500; Loglik: -13675; Acc.: 0.388; Alpha RMSE: 0.056; Beta RMSE: 0.146\n",
      "[Epoch  4900] ELBO: 14551; Loglik: -13755; Acc.: 0.388; Alpha RMSE: 0.073; Beta RMSE: 0.141\n",
      "[Epoch  5000] ELBO: 14559; Loglik: -13781; Acc.: 0.388; Alpha RMSE: 0.062; Beta RMSE: 0.154\n",
      "[Epoch  5100] ELBO: 14536; Loglik: -13765; Acc.: 0.388; Alpha RMSE: 0.052; Beta RMSE: 0.153\n",
      "[Epoch  5200] ELBO: 14493; Loglik: -13738; Acc.: 0.388; Alpha RMSE: 0.061; Beta RMSE: 0.161\n",
      "[Epoch  5300] ELBO: 14516; Loglik: -13798; Acc.: 0.388; Alpha RMSE: 0.049; Beta RMSE: 0.174\n",
      "[Epoch  5400] ELBO: 14500; Loglik: -13782; Acc.: 0.388; Alpha RMSE: 0.052; Beta RMSE: 0.166\n",
      "[Epoch  5500] ELBO: 14500; Loglik: -13808; Acc.: 0.388; Alpha RMSE: 0.056; Beta RMSE: 0.175\n",
      "[Epoch  5600] ELBO: 14484; Loglik: -13764; Acc.: 0.388; Alpha RMSE: 0.056; Beta RMSE: 0.168\n",
      "[Epoch  5700] ELBO: 14436; Loglik: -13770; Acc.: 0.388; Alpha RMSE: 0.077; Beta RMSE: 0.178\n",
      "[Epoch  5800] ELBO: 14426; Loglik: -13774; Acc.: 0.388; Alpha RMSE: 0.070; Beta RMSE: 0.165\n",
      "[Epoch  5900] ELBO: 14503; Loglik: -13836; Acc.: 0.388; Alpha RMSE: 0.084; Beta RMSE: 0.167\n",
      "[Epoch  6000] ELBO: 14458; Loglik: -13826; Acc.: 0.388; Alpha RMSE: 0.072; Beta RMSE: 0.179\n",
      "[Epoch  6100] ELBO: 14446; Loglik: -13815; Acc.: 0.388; Alpha RMSE: 0.072; Beta RMSE: 0.180\n",
      "[Epoch  6200] ELBO: 14447; Loglik: -13808; Acc.: 0.388; Alpha RMSE: 0.073; Beta RMSE: 0.188\n",
      "[Epoch  6300] ELBO: 14466; Loglik: -13852; Acc.: 0.388; Alpha RMSE: 0.073; Beta RMSE: 0.166\n",
      "[Epoch  6400] ELBO: 14487; Loglik: -13852; Acc.: 0.388; Alpha RMSE: 0.062; Beta RMSE: 0.182\n",
      "[Epoch  6500] ELBO: 14429; Loglik: -13842; Acc.: 0.388; Alpha RMSE: 0.072; Beta RMSE: 0.167\n",
      "[Epoch  6600] ELBO: 14475; Loglik: -13880; Acc.: 0.388; Alpha RMSE: 0.071; Beta RMSE: 0.175\n",
      "[Epoch  6700] ELBO: 14464; Loglik: -13859; Acc.: 0.388; Alpha RMSE: 0.074; Beta RMSE: 0.169\n",
      "[Epoch  6800] ELBO: 14483; Loglik: -13911; Acc.: 0.388; Alpha RMSE: 0.080; Beta RMSE: 0.187\n",
      "[Epoch  6900] ELBO: 14437; Loglik: -13877; Acc.: 0.388; Alpha RMSE: 0.072; Beta RMSE: 0.174\n",
      "Elapsed time: 89.52021908760071 \n",
      "\n",
      "True alpha: [ 1 -1]\n",
      "Est. alpha: [ 0.9847753 -0.8956241]\n",
      "\tBETA_X0: 0.985\n",
      "\tBETA_X1: -0.896\n",
      "\n",
      "True zeta: [ 1 -1]\n",
      "Est. zeta: [ 1.172284  -1.1767656]\n",
      "\tBETA_X2: 1.172\n",
      "\tBETA_X3: -1.177\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 59s, sys: 792 ms, total: 17min 59s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from core.torch_mxl import TorchMXL\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as td\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from core.lkj import LKJCholesky\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class TorchMXL_Ordered(TorchMXL):\n",
    "    def __init__(self, dcm_dataset, num_categories, batch_size, use_cuda=True, use_inference_net=False):\n",
    "        \"\"\"\n",
    "        Initializes the TorchMXL object.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dcm_dataset : Dataset\n",
    "            The choice dataset.\n",
    "        batch_size : int\n",
    "            The size of each batch of observations used during variational inference.\n",
    "        use_cuda : bool, optional\n",
    "            Whether or not to use GPU-acceleration with CUDA (default is True).\n",
    "        use_inference_net : bool, optional\n",
    "            Whether or not to use an inference network for amortizing the cost of variational inference (default is False).\n",
    "        \"\"\"\n",
    "        self.num_categories = num_categories\n",
    "        super().__init__(dcm_dataset, batch_size, use_cuda, use_inference_net)\n",
    "        \n",
    "        \n",
    "    def initialize_variational_distribution_q(self,):\n",
    "        # q(kappa) - initialize parameters of Normal approximation\n",
    "        self.kappa_mu = nn.Parameter(torch.randn(self.num_categories-2))\n",
    "        self.kappa_sigma = nn.Parameter(-1*torch.ones(self.num_categories-2))\n",
    "        \n",
    "        # q(alpha) - initialize mean and lower-cholesky factor of the covariance matrix\n",
    "        self.alpha_mu = nn.Parameter(torch.zeros(self.num_fixed_params))\n",
    "        self.alpha_cov_diag = nn.Parameter(torch.ones(self.num_fixed_params))\n",
    "        self.alpha_cov_offdiag = nn.Parameter(torch.zeros(int((self.num_fixed_params*(self.num_fixed_params-1))/2)))\n",
    "        self.tril_indices_alpha = torch.tril_indices(row=self.num_fixed_params, col=self.num_fixed_params, offset=-1)\n",
    "        \n",
    "        # q(zeta) - initialize mean and lower-cholesky factor of the covariance matrix\n",
    "        self.zeta_mu = nn.Parameter(torch.zeros(self.num_mixed_params))\n",
    "        self.zeta_cov_diag = nn.Parameter(torch.ones(self.num_mixed_params))\n",
    "        self.zeta_cov_offdiag = nn.Parameter(torch.zeros(int((self.num_mixed_params*(self.num_mixed_params-1))/2)))\n",
    "        self.tril_indices_zeta = torch.tril_indices(row=self.num_mixed_params, col=self.num_mixed_params, offset=-1)\n",
    "        \n",
    "        # q(Omega) - initialize means and variances of the diagonal and off-diagonal elements\n",
    "        #            of the lower-cholesky factor of the covariance matrix \n",
    "        self.L_omega_diag_mu = nn.Parameter(torch.ones(self.num_mixed_params))\n",
    "        self.L_omega_diag_sigma = nn.Parameter(torch.zeros(self.num_mixed_params))\n",
    "        self.L_omega_offdiag_mu = nn.Parameter(torch.ones(int((self.num_mixed_params*(self.num_mixed_params-1))/2)))\n",
    "        self.L_omega_offdiag_sigma = nn.Parameter(torch.zeros(int((self.num_mixed_params*(self.num_mixed_params-1))/2)))\n",
    "\n",
    "        # q(beta_n) - initialize mean and lower-cholesky factor of the covariance matrix\n",
    "        self.beta_mu = nn.Parameter(torch.zeros(self.num_resp, self.num_mixed_params))\n",
    "        self.beta_cov_diag = nn.Parameter(torch.ones(self.num_mixed_params))\n",
    "        self.beta_cov_offdiag = nn.Parameter(torch.zeros(int((self.num_mixed_params*(self.num_mixed_params-1))/2)))\n",
    "        self.tril_indices_zeta = torch.tril_indices(row=self.num_mixed_params, col=self.num_mixed_params, offset=-1)\n",
    "\n",
    "        if self.dcm_spec.model_type == 'ContextMXL':\n",
    "            # layers of neural net for context data\n",
    "            self.context_hidden_dim = 10\n",
    "            self.context_fc1 = nn.Linear(self.context.shape[-1], self.context_hidden_dim)\n",
    "            self.context_bn1 = nn.BatchNorm1d(self.context_hidden_dim)\n",
    "            self.context_fc2 = nn.Linear(self.context_hidden_dim, self.num_mixed_params + self.num_fixed_params)\n",
    "            self.context_dropout = nn.Dropout(0.5)\n",
    "\n",
    "        if self.use_inference_net:\n",
    "            # layers of inference neural network for amortization\n",
    "            self.kernel_size = self.num_params*self.num_alternatives+self.num_alternatives*2\n",
    "            self.infnet_hidden_dim = 200\n",
    "            self.cnn1 = torch.nn.Conv1d(1, self.infnet_hidden_dim, kernel_size=(self.kernel_size), stride=(self.kernel_size),\n",
    "                                        padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "            self.bn1 = nn.BatchNorm1d(1)\n",
    "            self.bn2 = nn.BatchNorm1d(self.infnet_hidden_dim)\n",
    "            self.fc1 = nn.Linear(self.infnet_hidden_dim, self.infnet_hidden_dim)\n",
    "            self.fc2mu = nn.Linear(self.infnet_hidden_dim, self.num_mixed_params)\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "            self.pooling = nn.MaxPool1d(int(self.num_menus), stride=(int(self.num_menus)))\n",
    "        \n",
    "                \n",
    "    def compute_variational_approximation_q(self, alt_attr, context_attr, obs_choices, alt_avail, alt_ids):\n",
    "        \"\"\"\n",
    "        Computes the variational approximation q(z) to the true posterior distribution of the model p(z|x), where z denotes the latent variables in the model (e.g., the fixed and random effect parameters) and x denotes the observed data (e.g., alternative attributes and observed choices). When the inference network is used to amortize the cost of variational inference, then this method passes the observations through the inference neural network in order to obtain an approximation of the posterior q(beta_n).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        alt_attr : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_alternatives*(num_fixed_attr+num_mixed_attr)) containing the attributes for the different alternatives.\n",
    "        context_attr : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_context_attributes) containing the attributes descrbing the context for the different choice situations.\n",
    "        obs_choices : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus) containing the observed choices (represented as integers in the set {0, ..., num_alternatives-1}).\n",
    "        alt_avail : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_alternatives) containing information about the availability of the different alternatives (represented as 0 or 1).\n",
    "        alt_ids : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_alternatives*(num_fixed_attr+num_mixed_attr)) mapping the attributes in alt_attr to the different alternatives (represented as integers in the set {0, ..., num_alternatives-1}).\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        q_alpha : Torch.distribution\n",
    "            Torch distribution with the posterior approximation over the global fixed effects preference parameters q(alpha)\n",
    "        q_zeta : Torch.distribution.\n",
    "            Torch distribution with the posterior approximation over the global mixed effects preference parameters q(zeta)\n",
    "        q_L_Omega_diag : Torch.distribution.\n",
    "            Torch distribution with the posterior approximation over the diagonal elements of the lower-Cholesky factorization of the covariance matrix q(Omega).\n",
    "        q_L_Omega_offdiag : Torch.distribution\n",
    "            Torch distribution with the posterior approximation over the off-diagonal elements of the lower-Cholesky factorization of the covariance matrix q(Omega).\n",
    "        q_beta : Torch.distribution\n",
    "            Torch distribution with the posterior approximation over the (local) per-respondent preference parameters q(beta_n) for each respondent n.\n",
    "        \"\"\"\n",
    "        \n",
    "        # q(kappa) - construct posterior approximation on tau_alpha\n",
    "        q_kappa = td.Normal(self.kappa_mu, self.softplus(self.kappa_sigma))\n",
    "        \n",
    "        # q(alpha) - construct posterior approximation on alpha\n",
    "        alpha_cov_tril = torch.zeros((self.num_fixed_params, self.num_fixed_params), device=self.device)\n",
    "        alpha_cov_tril[self.tril_indices_alpha[0], self.tril_indices_alpha[1]] = self.alpha_cov_offdiag\n",
    "        alpha_cov_tril += torch.diag_embed(self.softplus(self.alpha_cov_diag))\n",
    "        q_alpha = td.MultivariateNormal(self.alpha_mu, scale_tril=torch.tril(alpha_cov_tril))\n",
    "        \n",
    "        # q(zeta) - construct posterior approximation on zeta\n",
    "        zeta_cov_tril = torch.zeros((self.num_mixed_params, self.num_mixed_params), device=self.device)\n",
    "        zeta_cov_tril[self.tril_indices_zeta[0], self.tril_indices_zeta[1]] = self.zeta_cov_offdiag\n",
    "        zeta_cov_tril += torch.diag_embed(self.softplus(self.zeta_cov_diag))\n",
    "        q_zeta = td.MultivariateNormal(self.zeta_mu, scale_tril=torch.tril(zeta_cov_tril))\n",
    "        \n",
    "        # q(Omega) - construct posterior approximation on Omega using multiple independent Gaussians\n",
    "        q_L_Omega_diag = td.Normal(self.softplus(self.L_omega_diag_mu), self.softplus(self.L_omega_diag_sigma))\n",
    "        q_L_Omega_offdiag = td.Normal(self.L_omega_offdiag_mu, self.softplus(self.L_omega_offdiag_sigma))\n",
    "        \n",
    "        # q(beta_n) - construct posterior approximation on beta_n\n",
    "        beta_cov_tril = torch.zeros((self.num_mixed_params, self.num_mixed_params), device=self.device)\n",
    "        beta_cov_tril[self.tril_indices_zeta[0], self.tril_indices_zeta[1]] = self.beta_cov_offdiag\n",
    "        beta_cov_tril += torch.diag_embed(self.softplus(self.beta_cov_diag))\n",
    "        if self.use_inference_net:\n",
    "            # prepare input data for inference neural network\n",
    "            one_hot = torch.zeros(self.num_resp, self.num_menus, self.num_alternatives, device=self.device, dtype=torch.float)\n",
    "            one_hot = one_hot.scatter(2, obs_choices.unsqueeze(2).long(), 1)\n",
    "            inference_data = torch.cat([one_hot, alt_attr, alt_avail.float()], dim=-1)\n",
    "            inference_data = inference_data.flatten(1,2).unsqueeze(1)\n",
    "            \n",
    "            # compute the hidden units\n",
    "            hidden = self.bn1(inference_data)\n",
    "            hidden = self.cnn1(hidden)\n",
    "            hidden = self.relu(self.pooling(hidden))\n",
    "            hidden = self.bn2(hidden)\n",
    "            hidden = self.relu(self.fc1(hidden.flatten(1,2)))\n",
    "            mu_loc = self.fc2mu(hidden)\n",
    "            q_beta = td.MultivariateNormal(mu_loc, scale_tril=torch.tril(beta_cov_tril))\n",
    "        else:\n",
    "            q_beta = td.MultivariateNormal(self.beta_mu, scale_tril=torch.tril(beta_cov_tril))\n",
    "        \n",
    "        if self.dcm_spec.model_type == 'ContextMXL':\n",
    "            # pass context data through the context neural net\n",
    "            hidden = self.relu(self.context_fc1(context_attr))\n",
    "            hidden = self.context_dropout(hidden)\n",
    "            beta_offsets = self.context_fc2(hidden)\n",
    "            \n",
    "            return q_alpha, q_zeta, q_L_Omega_diag, q_L_Omega_offdiag, q_beta, beta_offsets\n",
    "        \n",
    "        return q_kappa, q_alpha, q_zeta, q_L_Omega_diag, q_L_Omega_offdiag, q_beta, None\n",
    "        \n",
    "        \n",
    "    def elbo(self, alt_attr, context_attr, obs_choices, alt_avail, obs_mask, alt_ids, indices):\n",
    "        \"\"\"\n",
    "        Computes the stochastic approximation to the evidence lower bound (ELBO) used by variational inference to optimize the parameters of the variational approximation q(z) to the true posterior distribution p(z|x).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        alt_attr : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_alternatives*(num_fixed_attr+num_mixed_attr)) containing the attributes for the different alternatives.\n",
    "        context_attr : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_context_attributes) containing the attributes descrbing the context for the different choice situations.\n",
    "        obs_choices : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus) containing the observed choices (represented as integers in the set {0, ..., num_alternatives-1}).\n",
    "        alt_avail : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_alternatives) containing information about the availability of the different alternatives (represented as 0 or 1).\n",
    "        obs_mask : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus) describing which menus in alt_attr and obs_choices are to be considered (represented as 0 or 1) - this is useful for panel data where different respondents have different numbers of choice situations.\n",
    "        alt_ids : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_alternatives*(num_fixed_attr+num_mixed_attr)) mapping the attributes in alt_attr to the different alternatives (represented as integers in the set {0, ..., num_alternatives-1}).\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        elbo : Torch.tensor\n",
    "            Value of the ELBO based on the current variational distribution q(z).\n",
    "        \"\"\"\n",
    "        \n",
    "        # ----- get posterior approximations -----\n",
    "        q_kappa, q_alpha, q_zeta, q_L_Omega_diag, q_L_Omega_offdiag, q_beta, beta_offsets = self.compute_variational_approximation_q(alt_attr, context_attr, obs_choices, alt_avail, alt_ids)\n",
    "        \n",
    "        # ----- sample from posterior approximations -----\n",
    "        kappa = q_kappa.rsample()\n",
    "        alpha = q_alpha.rsample()\n",
    "        zeta = q_zeta.rsample()\n",
    "        beta = q_beta.rsample()\n",
    "        L_Omega_diag = q_L_Omega_diag.rsample()\n",
    "        L_Omega_offdiag = q_L_Omega_offdiag.rsample()\n",
    "        L_Omega = torch.zeros((self.num_mixed_params, self.num_mixed_params), device=self.device)\n",
    "        L_Omega[self.tril_indices_zeta[0], self.tril_indices_zeta[1]] = L_Omega_offdiag\n",
    "        L_Omega += torch.diag_embed(self.softplus(L_Omega_diag))\n",
    "        \n",
    "        # ----- gather paramters for computing the utilities -----\n",
    "        beta_resp = self.gather_parameters_for_MNL_kernel(alpha, beta[indices])\n",
    "        \n",
    "        # ----- compute utilities -----\n",
    "        utilities = self.compute_utilities(beta_resp, alt_attr, alt_avail, alt_ids)\n",
    "        #print(\"utilities:\", utilities.shape) # torch.Size([5, 500, 1])\n",
    "        #print(utilities[0,0])\n",
    "\n",
    "        # ----- (expected) log-likelihood -----\n",
    "        \n",
    "        #print(\"kappa:\", kappa.shape)\n",
    "        cutoffs = torch.zeros(self.num_categories-1, device=self.device)\n",
    "        cutoffs[1:] = torch.cumsum(self.softplus(kappa), 0)\n",
    "        #cutoffs = torch.cumsum(self.softplus(kappa), 0)\n",
    "        #cutoffs = torch.tensor([1, 3, 5, 7], device=self.device)\n",
    "        \n",
    "        #print(\"cutoffs:\", cutoffs)\n",
    "        #print(cutoffs.repeat((self.num_resp,self.num_menus,1)).shape)\n",
    "        #print(cutoffs.repeat((self.num_resp,self.num_menus,1))[0,0])\n",
    "        cumprobs = torch.sigmoid(cutoffs.repeat((self.num_menus,self.num_resp,1)) - utilities)\n",
    "        #print(\"cumprobs:\", cumprobs.shape) # torch.Size([5, 500, 6])\n",
    "        #print(cumprobs[0,0])\n",
    "        probs = torch.zeros((self.num_menus,self.num_resp,self.num_categories), device=self.device)\n",
    "        probs[:,:,0] = cumprobs[:,:,0]\n",
    "        for i in range(1,self.num_categories-1):\n",
    "            probs[:,:,i] = cumprobs[:,:,i] - cumprobs[:,:,i-1]\n",
    "            \n",
    "        probs[:,:,self.num_categories-1] = 1 - cumprobs[:,:,self.num_categories-2]    \n",
    "        #print(\"probs:\", probs.shape) # torch.Size([5, 500, 6])\n",
    "        #print(probs[0,0])\n",
    "        #print(probs[0,0].sum())\n",
    "        #print(fail)\n",
    "        \n",
    "        \n",
    "        #probs = torch.zeros(self.num_categories, device=self.device)\n",
    "        #probs[0] = stable_sigmoid(true_cutoffs[0] - v[t])\n",
    "        #for j in range(1,k-1):\n",
    "        #    probs[j] = stable_sigmoid(true_cutoffs[j] - v[t]) - stable_sigmoid(true_cutoffs[j-1] - v[t])\n",
    "\n",
    "        #probs[k-1] = 1 - stable_sigmoid(true_cutoffs[k-2] - v[t])\n",
    "        \n",
    "        loglik = td.Categorical(probs=probs).log_prob(obs_choices.transpose(0,1))\n",
    "        loglik = torch.where(obs_mask.T, loglik, loglik.new_zeros(())) # use mask to filter out missing menus\n",
    "        loglik = loglik.sum()\n",
    "        #print(loglik)\n",
    "        \n",
    "        # ----- define priors -----\n",
    "        kappa_prior = td.Normal(torch.zeros(self.num_categories-2, device=self.device), \n",
    "                                torch.ones(self.num_categories-2, device=self.device))\n",
    "        \n",
    "        alpha_prior = td.MultivariateNormal(torch.zeros(self.num_fixed_params, device=self.device), \n",
    "                                           scale_tril=torch.tril(1*torch.eye(self.num_fixed_params, device=self.device)))\n",
    "        \n",
    "        zeta_prior = td.MultivariateNormal(torch.zeros(self.num_mixed_params, device=self.device), \n",
    "                                           scale_tril=torch.tril(1*torch.eye(self.num_mixed_params, device=self.device)))\n",
    "        \n",
    "        beta_prior = td.MultivariateNormal(zeta, scale_tril=L_Omega)\n",
    "\n",
    "        # vector of variances for each of the d variables - used in LKJ prior\n",
    "        theta_prior = td.HalfCauchy(1*torch.ones(self.num_mixed_params, device=self.device))\n",
    "\n",
    "        # lower cholesky factor of a correlation matrix\n",
    "        eta = 1*torch.ones(1, device=self.device)  # implies a uniform distribution over correlation matrices\n",
    "        L_Sigma_prior = LKJCholesky(self.num_mixed_params, eta)\n",
    "\n",
    "        # decompose L_Sigma into L_Omega and theta for scoring w.r.t. priors\n",
    "        Omega = torch.mm(L_Omega, L_Omega.T) \n",
    "        theta = torch.diag(Omega) \n",
    "        theta_sqrt = theta.sqrt()\n",
    "        L_Sigma = torch.mul(L_Omega / torch.outer(theta_sqrt, theta_sqrt), theta_sqrt)\n",
    "        \n",
    "        # ----- compute KL-divergence terms -----\n",
    "        kld = 0.\n",
    "        \n",
    "        # KL[q(kappa) || p(kappa)]\n",
    "        kld += td.kl_divergence(q_kappa, kappa_prior).sum()\n",
    "        \n",
    "        # KL[q(alpha) || p(alpha)]\n",
    "        kld += td.kl_divergence(q_alpha, alpha_prior)\n",
    "            \n",
    "        # KL[q(zeta) || p(zeta)]\n",
    "        kld += td.kl_divergence(q_zeta, zeta_prior)\n",
    "        \n",
    "        # KL[q(beta_n) || p(beta_n)]\n",
    "        kld += td.kl_divergence(q_beta, beta_prior).sum()\n",
    "        \n",
    "        # KL[q(Omega) || p(Omega)]\n",
    "        kld += q_L_Omega_diag.log_prob(L_Omega_diag).sum() + q_L_Omega_offdiag.log_prob(L_Omega_offdiag).sum() \n",
    "        kld += -L_Sigma_prior.log_prob(L_Sigma).sum() - theta_prior.log_prob(theta).sum()\n",
    "        \n",
    "        # ----- compute ELBO -----\n",
    "        # ELBO = -E[loglik] + KL[q || prior]\n",
    "        elbo = -loglik + kld\n",
    "        \n",
    "        # compute accuracy based on utilities\n",
    "        acc = utilities.argmax(-1) == obs_choices.transpose(0,1)\n",
    "        acc = torch.where(obs_mask.T, acc, acc.new_zeros(()))\n",
    "        acc = acc.sum() / obs_mask.sum()\n",
    "        \n",
    "        # remember values (e.g. to show progress)\n",
    "        self.loglik = loglik\n",
    "        self.kld = kld\n",
    "        self.acc = acc\n",
    "        \n",
    "        return elbo\n",
    "    \n",
    "    \n",
    "    def infer(self, num_epochs=10000, true_alpha=None, true_beta=None, true_beta_resp=None):\n",
    "        \"\"\"\n",
    "        Performs variational inference (amortized variational inference if use_inference_net is set to True). \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_epochs : int, optional\n",
    "            Number of passes/iterations through the dataset to be performed during ELBO maximization (default is 10000).\n",
    "        true_alpha : np.array, optional\n",
    "            Numpy array with true values of the global fixed-effect preference parameters for comparison (useful for investigating the progress of variational inference in cases when the true values of the preference parameters are known). If provided, then this method outputs additional information during ELBO maximization.\n",
    "        true_beta : np.array, optional\n",
    "            Numpy array with true values of the global random-effect preference parameters for comparison (useful for investigating the progress of variational inference in cases when the true values of the preference parameters are known). If provided, then this method outputs additional information during ELBO maximization.\n",
    "        true_beta_resp : np.array, optional\n",
    "            Numpy array with true values of the per-respondent preference parameters for comparison (useful for investigating the progress of variational inference in cases when the true values of the preference parameters are known). If provided, then this method outputs additional information during ELBO maximization.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        results : dict\n",
    "            Python dictionary containing the results of variational inference. \n",
    "        \"\"\"\n",
    "        self.to(self.device)\n",
    "        \n",
    "        #print(\"Initial ELBO value: %.1f\" % self.loss(self.train_x, self.context_info, self.train_y, self.alt_av_mat_cuda, self.mask_cuda, self.alt_ids_cuda).item())\n",
    "\n",
    "        optimizer = Adam(self.parameters(), lr=1e-2)\n",
    "\n",
    "        self.train() # enable training mode\n",
    "        \n",
    "        tic = time.time()\n",
    "        alpha_errors = []\n",
    "        beta_errors = []\n",
    "        betaInd_errors = []\n",
    "        for epoch in range(num_epochs):\n",
    "            permutation = torch.randperm(self.num_resp)\n",
    "            \n",
    "            for i in range(0, self.num_resp, self.batch_size):\n",
    "                \n",
    "                indices = permutation[i:i+self.batch_size]\n",
    "                batch_x, batch_context, batch_y = self.train_x[indices], self.context_info[indices], self.train_y[indices]\n",
    "                batch_alt_av_mat, batch_mask_cuda, batch_alt_ids = self.alt_av_mat_cuda[indices], self.mask_cuda[indices], self.alt_ids_cuda[indices]\n",
    "                \n",
    "                batch_x = batch_x.to(self.device)\n",
    "                batch_context = batch_context.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                batch_alt_av_mat = batch_alt_av_mat.to(self.device)\n",
    "                batch_mask_cuda = batch_mask_cuda.to(self.device)\n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                elbo = self.elbo(batch_x, batch_context, batch_y, batch_alt_av_mat, batch_mask_cuda, batch_alt_ids, indices)\n",
    "\n",
    "                elbo.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if not epoch % 100:\n",
    "                    msg = \"[Epoch %5d] ELBO: %.0f; Loglik: %.0f; Acc.: %.3f\" % (epoch, elbo.item(), self.loglik, self.acc)\n",
    "                    if np.all(true_alpha != None):\n",
    "                        alpha_rmse = np.sqrt(np.mean((true_alpha - self.alpha_mu.detach().cpu().numpy())**2)) if len(true_alpha) > 0 else np.inf\n",
    "                        alpha_errors += [alpha_rmse]\n",
    "                        msg += \"; Alpha RMSE: %.3f\" % (alpha_rmse,)\n",
    "                    if self.dcm_spec.model_type != 'MNL' and np.all(true_beta != None):\n",
    "                        beta_rmse = np.sqrt(np.mean((true_beta - self.zeta_mu.detach().cpu().numpy())**2)) if len(true_beta) > 0 else np.inf\n",
    "                        beta_errors += [beta_rmse]\n",
    "                        msg += \"; Beta RMSE: %.3f\" % (beta_rmse,)\n",
    "                    if self.dcm_spec.model_type != 'MNL' and np.all(true_beta_resp != None):\n",
    "                        params_resps_rmse = np.sqrt(np.mean((true_beta_resp - self.beta_mu.detach().cpu().numpy())**2)) if len(true_beta_resp) > 0 else np.inf\n",
    "                        betaInd_errors += [params_resps_rmse]\n",
    "                        msg += \"; BetaInd RMSE: %.3f\" % (params_resps_rmse,)\n",
    "\n",
    "                    print(msg)\n",
    "\n",
    "        toc = time.time() - tic\n",
    "        print('Elapsed time:', toc, '\\n')\n",
    "            \n",
    "        # prepare python dictionary of results to output\n",
    "        results = {}\n",
    "        results[\"Estimation time\"] = toc\n",
    "        results[\"Est. alpha\"] = self.alpha_mu.detach().cpu().numpy()\n",
    "        if self.dcm_spec.model_type != 'MNL':\n",
    "            results[\"Est. zeta\"] = self.zeta_mu.detach().cpu().numpy()\n",
    "            if self.use_inference_net: \n",
    "                q_alpha, q_zeta, q_L_Sigma_diag, q_L_Sigma_offdiag, q_beta, beta_offsets = self.forward(self.train_x.to(self.device), self.context_info.to(self.device), self.train_y.to(self.device), self.alt_av_mat_cuda.to(self.device), self.alt_ids_cuda.to(self.device))\n",
    "                results[\"Est. beta_n\"] = q_beta.loc.detach().cpu().numpy()\n",
    "            else:\n",
    "                results[\"Est. beta_n\"] = self.beta_mu.detach().cpu().numpy()\n",
    "        results[\"ELBO\"] = elbo.item()\n",
    "        results[\"Loglikelihood\"] = self.loglik.item()\n",
    "        results[\"Accuracy\"] = self.acc.item()\n",
    "        \n",
    "        # show quick summary of results\n",
    "        if np.all(true_alpha != None): print(\"True alpha:\", true_alpha)\n",
    "        print(\"Est. alpha:\", self.alpha_mu.detach().cpu().numpy())\n",
    "        \n",
    "        for i in range(len(self.dcm_spec.fixed_param_names)):\n",
    "            print(\"\\t%s: %.3f\" % (self.dcm_spec.fixed_param_names[i], results[\"Est. alpha\"][i]))\n",
    "        print()\n",
    "\n",
    "        if self.dcm_spec.model_type != 'MNL':\n",
    "            if np.all(true_beta != None): print(\"True zeta:\", true_beta)\n",
    "            print(\"Est. zeta:\", self.zeta_mu.detach().cpu().numpy())\n",
    "        \n",
    "            for i in range(len(self.dcm_spec.mixed_param_names)):\n",
    "                print(\"\\t%s: %.3f\" % (self.dcm_spec.mixed_param_names[i], results[\"Est. zeta\"][i]))\n",
    "            print()\n",
    "\n",
    "        if np.all(true_alpha != None) or np.all(true_beta != None) or np.all(true_beta_resp != None):\n",
    "            if np.all(true_alpha != None): plt.plot(alpha_errors)\n",
    "            if np.all(true_beta != None): plt.plot(beta_errors)\n",
    "            if np.all(true_beta_resp != None): plt.plot(betaInd_errors)\n",
    "            plt.legend(['alpha rmse','beta_rmse','beta_resps_rmse'])\n",
    "            plt.show();\n",
    "        \n",
    "        return results\n",
    "        \n",
    "\n",
    "# instantiate MXL model\n",
    "num_categories = 5\n",
    "mxl = TorchMXL_Ordered(dcm_dataset, num_categories, batch_size=num_resp, use_inference_net=False, use_cuda=True)\n",
    "\n",
    "# True beta: [ 1 -1  1 -1]\n",
    "\n",
    "# run Bayesian inference (variational inference)\n",
    "results = mxl.infer(num_epochs=7000, true_alpha=np.array([1, -1]), true_beta=np.array([1, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True alpha: [-0.7 -0.2  0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.3435,  0.3848,  1.5897], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mxl.kappa_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6234, -0.1010,  0.5740], device='cuda:0', grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rico uses an exp() transform instead of softplus(), so we need to convert to make it comparable\n",
    "torch.log(mxl.softplus(mxl.kappa_mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"results\" dictionary containts a summary of the results of variational inference, including means of the posterior approximations for the different parameters in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Estimation time': 64.12379693984985,\n",
       " 'Est. alpha': array([ 1.0848361, -0.9402135], dtype=float32),\n",
       " 'Est. zeta': array([ 1.0654774, -1.1859826], dtype=float32),\n",
       " 'Est. beta_n': array([[ 0.7436627 , -1.5113661 ],\n",
       "        [ 1.1884117 , -1.5241228 ],\n",
       "        [ 0.5740844 , -1.2508584 ],\n",
       "        ...,\n",
       "        [ 0.94637936, -1.5531573 ],\n",
       "        [ 1.308997  , -1.1411221 ],\n",
       "        [ 1.1743374 , -1.6200343 ]], dtype=float32),\n",
       " 'ELBO': 14481.8515625,\n",
       " 'Loglikelihood': -13628.396484375,\n",
       " 'Accuracy': 0.38760000467300415}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interface is currently being improved to include additional output information, but additional information can be obtained from the attributes of the \"mxl\" object for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo of SVI using simulated data for 500 individuals\n",
    "\n",
    "We begin by performing the necessary imports:\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate simulated data\n",
    "\n",
    "We predefine the fixed effects parameters (true_alpha) and random effects parameters (true_beta), as well as the covariance matrix (true_Omega), and sample simulated choice data for 500 respondents (num_resp), each with 5 choice situations (num_menus). The number of choice alternatives is set to 5. \n",
    "\n",
    "df = pd.read_csv('fake_data_rico.csv', index_col=0)\n",
    "num_resp = len(df)\n",
    "df['indID'] = np.arange(num_resp)\n",
    "df.head()\n",
    "\n",
    "# Mixed Logit specification\n",
    "\n",
    "We now make use of the developed formula interface to specify the utilities of the mixed logit model. \n",
    "\n",
    "We begin by defining the fixed effects parameters, the random effects parameters, and the observed variables. This creates instances of Python objects that can be put together to define the utility functions for the different alternatives.\n",
    "\n",
    "Once the utilities are defined, we collect them in a Python dictionary mapping alternative names to their corresponding expressions.\n",
    "\n",
    "from core.dcm_interface import FixedEffect, RandomEffect, ObservedVariable\n",
    "import torch.distributions as dists\n",
    "\n",
    "# define fixed effects parameters\n",
    "B_X0 = FixedEffect('BETA_X0')\n",
    "B_X1 = FixedEffect('BETA_X1')\n",
    "\n",
    "# define random effects parameters\n",
    "B_X2 = RandomEffect('BETA_X2')\n",
    "B_X3 = RandomEffect('BETA_X3')\n",
    "\n",
    "# define observed variables\n",
    "for attr in df.columns:\n",
    "    exec(\"%s = ObservedVariable('%s')\" % (attr,attr))\n",
    "\n",
    "# define utility functions\n",
    "V1 = B_X0*x0 + B_X1*x1 + B_X2*x2 + B_X3*x3\n",
    "\n",
    "# associate utility functions with the names of the alternatives\n",
    "utilities = {\"ALT1\": V1}\n",
    "\n",
    "We are now ready to create a Specification object containing the utilities that we have just defined. Note that we must also specify the type of choice model to be used - a mixed logit model (MXL) in this case.\n",
    "\n",
    "Note that we can inspect the specification by printing the dcm_spec object.\n",
    "\n",
    "from core.dcm_interface import Specification\n",
    "\n",
    "#Logit(choice, utilities, availability, df)\n",
    "#Logit(choice_test, utilities, availability_test, df_test)\n",
    "\n",
    "# create MXL specification object based on the utilities previously defined\n",
    "dcm_spec = Specification('MXL', utilities)\n",
    "print(dcm_spec)\n",
    "\n",
    "Once the Specification is defined, we need to define the DCM Dataset object that goes along with it. For this, we instantiate the Dataset class with the Pandas dataframe containing the data in the so-called \"wide format\", the name of column in the dataframe containing the observed choices and the dcm_spec that we have previously created.\n",
    "\n",
    "Note that since this is panel data, we must also specify the name of the column in the dataframe that contains the ID of the respondent (this should be a integer ranging from 0 the num_resp-1).\n",
    "\n",
    "from core.dcm_interface import Dataset\n",
    "\n",
    "# create DCM dataset object\n",
    "dcm_dataset = Dataset(df, 'choice', dcm_spec, resp_id_col='indID')\n",
    "\n",
    "As with the specification, we can inspect the DCM dataset by printing the dcm_dataset object:\n",
    "\n",
    "print(dcm_dataset)\n",
    "\n",
    "# Bayesian Mixed Logit Model in PyTorch\n",
    "\n",
    "It is now time to perform approximate Bayesian inference on the mixed logit model that we have specified. The generative process of the MXL model that we will be using is the following:\n",
    "\n",
    "1. Draw fixed taste parameters $\\boldsymbol\\alpha \\sim \\mathcal{N}(\\boldsymbol\\lambda_0, \\boldsymbol\\Xi_0)$\n",
    "2. Draw mean vector $\\boldsymbol\\zeta \\sim \\mathcal{N}(\\boldsymbol\\mu_0, \\boldsymbol\\Sigma_0)$\n",
    "3. Draw scales vector $\\boldsymbol\\theta \\sim \\mbox{half-Cauchy}(\\boldsymbol\\sigma_0)$\n",
    "4. Draw correlation matrix $\\boldsymbol\\Psi \\sim \\mbox{LKJ}(\\nu)$\n",
    "5. For each decision-maker $n \\in \\{1,\\dots,N\\}$\n",
    "    1. Draw random taste parameters $\\boldsymbol\\beta_n \\sim \\mathcal{N}(\\boldsymbol\\zeta,\\boldsymbol\\Omega)$\n",
    "    2. For each choice occasion $t \\in \\{1,\\dots,T_n\\}$\n",
    "        1. Draw observed choice $y_{nt} \\sim \\mbox{MNL}(\\boldsymbol\\alpha, \\boldsymbol\\beta_n, \\textbf{X}_{nt})$\n",
    "        \n",
    "where $\\boldsymbol\\Omega = \\mbox{diag}(\\boldsymbol\\theta) \\times \\boldsymbol\\Psi \\times  \\mbox{diag}(\\boldsymbol\\theta)$.\n",
    "\n",
    "We can instantiate this model from the TorchMXL using the following code. We can the run variational inference to approximate the posterior distribution of the latent variables in the model. Note that since in this case we know the true parameters that were used to generate the simualated choice data, we can pass them to the \"infer\" method in order to obtain additional information during the ELBO maximization (useful for tracking the progress of VI and for other debugging purposes). \n",
    "\n",
    "%%time\n",
    "\n",
    "from core.torch_mxl import TorchMXL\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as td\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from core.lkj import LKJCholesky\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class TorchMXL_Ordered(TorchMXL):\n",
    "    def __init__(self, dcm_dataset, num_categories, batch_size, use_cuda=True, use_inference_net=False):\n",
    "        \"\"\"\n",
    "        Initializes the TorchMXL object.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dcm_dataset : Dataset\n",
    "            The choice dataset.\n",
    "        batch_size : int\n",
    "            The size of each batch of observations used during variational inference.\n",
    "        use_cuda : bool, optional\n",
    "            Whether or not to use GPU-acceleration with CUDA (default is True).\n",
    "        use_inference_net : bool, optional\n",
    "            Whether or not to use an inference network for amortizing the cost of variational inference (default is False).\n",
    "        \"\"\"\n",
    "        self.num_categories = num_categories\n",
    "        super().__init__(dcm_dataset, batch_size, use_cuda, use_inference_net)\n",
    "        \n",
    "        \n",
    "    def initialize_variational_distribution_q(self,):\n",
    "        # q(kappa) - initialize parameters of Normal approximation\n",
    "        self.kappa_mu = nn.Parameter(torch.randn(self.num_categories-2))\n",
    "        self.kappa_sigma = nn.Parameter(-1*torch.ones(self.num_categories-2))\n",
    "        \n",
    "        # q(alpha) - initialize mean and lower-cholesky factor of the covariance matrix\n",
    "        self.alpha_mu = nn.Parameter(torch.zeros(self.num_fixed_params))\n",
    "        self.alpha_cov_diag = nn.Parameter(torch.ones(self.num_fixed_params))\n",
    "        self.alpha_cov_offdiag = nn.Parameter(torch.zeros(int((self.num_fixed_params*(self.num_fixed_params-1))/2)))\n",
    "        self.tril_indices_alpha = torch.tril_indices(row=self.num_fixed_params, col=self.num_fixed_params, offset=-1)\n",
    "        \n",
    "        # q(zeta) - initialize mean and lower-cholesky factor of the covariance matrix\n",
    "        self.zeta_mu = nn.Parameter(torch.zeros(self.num_mixed_params))\n",
    "        self.zeta_cov_diag = nn.Parameter(torch.ones(self.num_mixed_params))\n",
    "        self.zeta_cov_offdiag = nn.Parameter(torch.zeros(int((self.num_mixed_params*(self.num_mixed_params-1))/2)))\n",
    "        self.tril_indices_zeta = torch.tril_indices(row=self.num_mixed_params, col=self.num_mixed_params, offset=-1)\n",
    "        \n",
    "        # q(Omega) - initialize means and variances of the diagonal and off-diagonal elements\n",
    "        #            of the lower-cholesky factor of the covariance matrix \n",
    "        self.L_omega_diag_mu = nn.Parameter(torch.ones(self.num_mixed_params))\n",
    "        self.L_omega_diag_sigma = nn.Parameter(torch.zeros(self.num_mixed_params))\n",
    "        self.L_omega_offdiag_mu = nn.Parameter(torch.ones(int((self.num_mixed_params*(self.num_mixed_params-1))/2)))\n",
    "        self.L_omega_offdiag_sigma = nn.Parameter(torch.zeros(int((self.num_mixed_params*(self.num_mixed_params-1))/2)))\n",
    "\n",
    "        # q(beta_n) - initialize mean and lower-cholesky factor of the covariance matrix\n",
    "        self.beta_mu = nn.Parameter(torch.zeros(self.num_resp, self.num_mixed_params))\n",
    "        self.beta_cov_diag = nn.Parameter(torch.ones(self.num_mixed_params))\n",
    "        self.beta_cov_offdiag = nn.Parameter(torch.zeros(int((self.num_mixed_params*(self.num_mixed_params-1))/2)))\n",
    "        self.tril_indices_zeta = torch.tril_indices(row=self.num_mixed_params, col=self.num_mixed_params, offset=-1)\n",
    "\n",
    "        if self.dcm_spec.model_type == 'ContextMXL':\n",
    "            # layers of neural net for context data\n",
    "            self.context_hidden_dim = 10\n",
    "            self.context_fc1 = nn.Linear(self.context.shape[-1], self.context_hidden_dim)\n",
    "            self.context_bn1 = nn.BatchNorm1d(self.context_hidden_dim)\n",
    "            self.context_fc2 = nn.Linear(self.context_hidden_dim, self.num_mixed_params + self.num_fixed_params)\n",
    "            self.context_dropout = nn.Dropout(0.5)\n",
    "\n",
    "        if self.use_inference_net:\n",
    "            # layers of inference neural network for amortization\n",
    "            self.kernel_size = self.num_params*self.num_alternatives+self.num_alternatives*2\n",
    "            self.infnet_hidden_dim = 200\n",
    "            self.cnn1 = torch.nn.Conv1d(1, self.infnet_hidden_dim, kernel_size=(self.kernel_size), stride=(self.kernel_size),\n",
    "                                        padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "            self.bn1 = nn.BatchNorm1d(1)\n",
    "            self.bn2 = nn.BatchNorm1d(self.infnet_hidden_dim)\n",
    "            self.fc1 = nn.Linear(self.infnet_hidden_dim, self.infnet_hidden_dim)\n",
    "            self.fc2mu = nn.Linear(self.infnet_hidden_dim, self.num_mixed_params)\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "            self.pooling = nn.MaxPool1d(int(self.num_menus), stride=(int(self.num_menus)))\n",
    "        \n",
    "                \n",
    "    def compute_variational_approximation_q(self, alt_attr, context_attr, obs_choices, alt_avail, alt_ids):\n",
    "        \"\"\"\n",
    "        Computes the variational approximation q(z) to the true posterior distribution of the model p(z|x), where z denotes the latent variables in the model (e.g., the fixed and random effect parameters) and x denotes the observed data (e.g., alternative attributes and observed choices). When the inference network is used to amortize the cost of variational inference, then this method passes the observations through the inference neural network in order to obtain an approximation of the posterior q(beta_n).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        alt_attr : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_alternatives*(num_fixed_attr+num_mixed_attr)) containing the attributes for the different alternatives.\n",
    "        context_attr : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_context_attributes) containing the attributes descrbing the context for the different choice situations.\n",
    "        obs_choices : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus) containing the observed choices (represented as integers in the set {0, ..., num_alternatives-1}).\n",
    "        alt_avail : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_alternatives) containing information about the availability of the different alternatives (represented as 0 or 1).\n",
    "        alt_ids : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_alternatives*(num_fixed_attr+num_mixed_attr)) mapping the attributes in alt_attr to the different alternatives (represented as integers in the set {0, ..., num_alternatives-1}).\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        q_alpha : Torch.distribution\n",
    "            Torch distribution with the posterior approximation over the global fixed effects preference parameters q(alpha)\n",
    "        q_zeta : Torch.distribution.\n",
    "            Torch distribution with the posterior approximation over the global mixed effects preference parameters q(zeta)\n",
    "        q_L_Omega_diag : Torch.distribution.\n",
    "            Torch distribution with the posterior approximation over the diagonal elements of the lower-Cholesky factorization of the covariance matrix q(Omega).\n",
    "        q_L_Omega_offdiag : Torch.distribution\n",
    "            Torch distribution with the posterior approximation over the off-diagonal elements of the lower-Cholesky factorization of the covariance matrix q(Omega).\n",
    "        q_beta : Torch.distribution\n",
    "            Torch distribution with the posterior approximation over the (local) per-respondent preference parameters q(beta_n) for each respondent n.\n",
    "        \"\"\"\n",
    "        \n",
    "        # q(kappa) - construct posterior approximation on tau_alpha\n",
    "        q_kappa = td.Normal(self.kappa_mu, self.softplus(self.kappa_sigma))\n",
    "        \n",
    "        # q(alpha) - construct posterior approximation on alpha\n",
    "        alpha_cov_tril = torch.zeros((self.num_fixed_params, self.num_fixed_params), device=self.device)\n",
    "        alpha_cov_tril[self.tril_indices_alpha[0], self.tril_indices_alpha[1]] = self.alpha_cov_offdiag\n",
    "        alpha_cov_tril += torch.diag_embed(self.softplus(self.alpha_cov_diag))\n",
    "        q_alpha = td.MultivariateNormal(self.alpha_mu, scale_tril=torch.tril(alpha_cov_tril))\n",
    "        \n",
    "        # q(zeta) - construct posterior approximation on zeta\n",
    "        zeta_cov_tril = torch.zeros((self.num_mixed_params, self.num_mixed_params), device=self.device)\n",
    "        zeta_cov_tril[self.tril_indices_zeta[0], self.tril_indices_zeta[1]] = self.zeta_cov_offdiag\n",
    "        zeta_cov_tril += torch.diag_embed(self.softplus(self.zeta_cov_diag))\n",
    "        q_zeta = td.MultivariateNormal(self.zeta_mu, scale_tril=torch.tril(zeta_cov_tril))\n",
    "        \n",
    "        # q(Omega) - construct posterior approximation on Omega using multiple independent Gaussians\n",
    "        q_L_Omega_diag = td.Normal(self.softplus(self.L_omega_diag_mu), self.softplus(self.L_omega_diag_sigma))\n",
    "        q_L_Omega_offdiag = td.Normal(self.L_omega_offdiag_mu, self.softplus(self.L_omega_offdiag_sigma))\n",
    "        \n",
    "        # q(beta_n) - construct posterior approximation on beta_n\n",
    "        beta_cov_tril = torch.zeros((self.num_mixed_params, self.num_mixed_params), device=self.device)\n",
    "        beta_cov_tril[self.tril_indices_zeta[0], self.tril_indices_zeta[1]] = self.beta_cov_offdiag\n",
    "        beta_cov_tril += torch.diag_embed(self.softplus(self.beta_cov_diag))\n",
    "        if self.use_inference_net:\n",
    "            # prepare input data for inference neural network\n",
    "            one_hot = torch.zeros(self.num_resp, self.num_menus, self.num_alternatives, device=self.device, dtype=torch.float)\n",
    "            one_hot = one_hot.scatter(2, obs_choices.unsqueeze(2).long(), 1)\n",
    "            inference_data = torch.cat([one_hot, alt_attr, alt_avail.float()], dim=-1)\n",
    "            inference_data = inference_data.flatten(1,2).unsqueeze(1)\n",
    "            \n",
    "            # compute the hidden units\n",
    "            hidden = self.bn1(inference_data)\n",
    "            hidden = self.cnn1(hidden)\n",
    "            hidden = self.relu(self.pooling(hidden))\n",
    "            hidden = self.bn2(hidden)\n",
    "            hidden = self.relu(self.fc1(hidden.flatten(1,2)))\n",
    "            mu_loc = self.fc2mu(hidden)\n",
    "            q_beta = td.MultivariateNormal(mu_loc, scale_tril=torch.tril(beta_cov_tril))\n",
    "        else:\n",
    "            q_beta = td.MultivariateNormal(self.beta_mu, scale_tril=torch.tril(beta_cov_tril))\n",
    "        \n",
    "        if self.dcm_spec.model_type == 'ContextMXL':\n",
    "            # pass context data through the context neural net\n",
    "            hidden = self.relu(self.context_fc1(context_attr))\n",
    "            hidden = self.context_dropout(hidden)\n",
    "            beta_offsets = self.context_fc2(hidden)\n",
    "            \n",
    "            return q_alpha, q_zeta, q_L_Omega_diag, q_L_Omega_offdiag, q_beta, beta_offsets\n",
    "        \n",
    "        return q_kappa, q_alpha, q_zeta, q_L_Omega_diag, q_L_Omega_offdiag, q_beta, None\n",
    "        \n",
    "        \n",
    "    def elbo(self, alt_attr, context_attr, obs_choices, alt_avail, obs_mask, alt_ids, indices):\n",
    "        \"\"\"\n",
    "        Computes the stochastic approximation to the evidence lower bound (ELBO) used by variational inference to optimize the parameters of the variational approximation q(z) to the true posterior distribution p(z|x).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        alt_attr : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_alternatives*(num_fixed_attr+num_mixed_attr)) containing the attributes for the different alternatives.\n",
    "        context_attr : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_context_attributes) containing the attributes descrbing the context for the different choice situations.\n",
    "        obs_choices : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus) containing the observed choices (represented as integers in the set {0, ..., num_alternatives-1}).\n",
    "        alt_avail : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_alternatives) containing information about the availability of the different alternatives (represented as 0 or 1).\n",
    "        obs_mask : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus) describing which menus in alt_attr and obs_choices are to be considered (represented as 0 or 1) - this is useful for panel data where different respondents have different numbers of choice situations.\n",
    "        alt_ids : Torch.tensor\n",
    "            Torch tensor of shape (batch_size, num_menus, num_alternatives*(num_fixed_attr+num_mixed_attr)) mapping the attributes in alt_attr to the different alternatives (represented as integers in the set {0, ..., num_alternatives-1}).\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        elbo : Torch.tensor\n",
    "            Value of the ELBO based on the current variational distribution q(z).\n",
    "        \"\"\"\n",
    "        \n",
    "        # ----- get posterior approximations -----\n",
    "        q_kappa, q_alpha, q_zeta, q_L_Omega_diag, q_L_Omega_offdiag, q_beta, beta_offsets = self.compute_variational_approximation_q(alt_attr, context_attr, obs_choices, alt_avail, alt_ids)\n",
    "        \n",
    "        # ----- sample from posterior approximations -----\n",
    "        kappa = q_kappa.rsample()\n",
    "        alpha = q_alpha.rsample()\n",
    "        zeta = q_zeta.rsample()\n",
    "        beta = q_beta.rsample()\n",
    "        L_Omega_diag = q_L_Omega_diag.rsample()\n",
    "        L_Omega_offdiag = q_L_Omega_offdiag.rsample()\n",
    "        L_Omega = torch.zeros((self.num_mixed_params, self.num_mixed_params), device=self.device)\n",
    "        L_Omega[self.tril_indices_zeta[0], self.tril_indices_zeta[1]] = L_Omega_offdiag\n",
    "        L_Omega += torch.diag_embed(self.softplus(L_Omega_diag))\n",
    "        \n",
    "        # ----- gather paramters for computing the utilities -----\n",
    "        beta_resp = self.gather_parameters_for_MNL_kernel(alpha, beta[indices])\n",
    "        \n",
    "        # ----- compute utilities -----\n",
    "        utilities = self.compute_utilities(beta_resp, alt_attr, alt_avail, alt_ids)\n",
    "        #print(\"utilities:\", utilities.shape) # torch.Size([5, 500, 1])\n",
    "        #print(utilities[0,0])\n",
    "\n",
    "        # ----- (expected) log-likelihood -----\n",
    "        \n",
    "        #print(\"kappa:\", kappa.shape)\n",
    "        cutoffs = torch.zeros(self.num_categories-1, device=self.device)\n",
    "        cutoffs[1:] = torch.cumsum(self.softplus(kappa), 0)\n",
    "        #cutoffs = torch.cumsum(self.softplus(kappa), 0)\n",
    "        #cutoffs = torch.tensor([1, 3, 5, 7], device=self.device)\n",
    "        \n",
    "        #print(\"cutoffs:\", cutoffs)\n",
    "        #print(cutoffs.repeat((self.num_resp,self.num_menus,1)).shape)\n",
    "        #print(cutoffs.repeat((self.num_resp,self.num_menus,1))[0,0])\n",
    "        cumprobs = torch.sigmoid(cutoffs.repeat((self.num_menus,self.num_resp,1)) - utilities)\n",
    "        #print(\"cumprobs:\", cumprobs.shape) # torch.Size([5, 500, 6])\n",
    "        #print(cumprobs[0,0])\n",
    "        probs = torch.zeros((self.num_menus,self.num_resp,self.num_categories), device=self.device)\n",
    "        probs[:,:,0] = cumprobs[:,:,0]\n",
    "        for i in range(1,self.num_categories-1):\n",
    "            probs[:,:,i] = cumprobs[:,:,i] - cumprobs[:,:,i-1]\n",
    "            \n",
    "        probs[:,:,self.num_categories-1] = 1 - cumprobs[:,:,self.num_categories-2]    \n",
    "        #print(\"probs:\", probs.shape) # torch.Size([5, 500, 6])\n",
    "        #print(probs[0,0])\n",
    "        #print(probs[0,0].sum())\n",
    "        #print(fail)\n",
    "        \n",
    "        \n",
    "        #probs = torch.zeros(self.num_categories, device=self.device)\n",
    "        #probs[0] = stable_sigmoid(true_cutoffs[0] - v[t])\n",
    "        #for j in range(1,k-1):\n",
    "        #    probs[j] = stable_sigmoid(true_cutoffs[j] - v[t]) - stable_sigmoid(true_cutoffs[j-1] - v[t])\n",
    "\n",
    "        #probs[k-1] = 1 - stable_sigmoid(true_cutoffs[k-2] - v[t])\n",
    "        \n",
    "        loglik = td.Categorical(probs=probs).log_prob(obs_choices.transpose(0,1))\n",
    "        loglik = torch.where(obs_mask.T, loglik, loglik.new_zeros(())) # use mask to filter out missing menus\n",
    "        loglik = loglik.sum()\n",
    "        #print(loglik)\n",
    "        \n",
    "        # ----- define priors -----\n",
    "        kappa_prior = td.Normal(torch.zeros(self.num_categories-2, device=self.device), \n",
    "                                torch.ones(self.num_categories-2, device=self.device))\n",
    "        \n",
    "        alpha_prior = td.MultivariateNormal(torch.zeros(self.num_fixed_params, device=self.device), \n",
    "                                           scale_tril=torch.tril(1*torch.eye(self.num_fixed_params, device=self.device)))\n",
    "        \n",
    "        zeta_prior = td.MultivariateNormal(torch.zeros(self.num_mixed_params, device=self.device), \n",
    "                                           scale_tril=torch.tril(1*torch.eye(self.num_mixed_params, device=self.device)))\n",
    "        \n",
    "        beta_prior = td.MultivariateNormal(zeta, scale_tril=L_Omega)\n",
    "\n",
    "        # vector of variances for each of the d variables - used in LKJ prior\n",
    "        theta_prior = td.HalfCauchy(1*torch.ones(self.num_mixed_params, device=self.device))\n",
    "\n",
    "        # lower cholesky factor of a correlation matrix\n",
    "        eta = 1*torch.ones(1, device=self.device)  # implies a uniform distribution over correlation matrices\n",
    "        L_Sigma_prior = LKJCholesky(self.num_mixed_params, eta)\n",
    "\n",
    "        # decompose L_Sigma into L_Omega and theta for scoring w.r.t. priors\n",
    "        Omega = torch.mm(L_Omega, L_Omega.T) \n",
    "        theta = torch.diag(Omega) \n",
    "        theta_sqrt = theta.sqrt()\n",
    "        L_Sigma = torch.mul(L_Omega / torch.outer(theta_sqrt, theta_sqrt), theta_sqrt)\n",
    "        \n",
    "        # ----- compute KL-divergence terms -----\n",
    "        kld = 0.\n",
    "        \n",
    "        # KL[q(kappa) || p(kappa)]\n",
    "        kld += td.kl_divergence(q_kappa, kappa_prior).sum()\n",
    "        \n",
    "        # KL[q(alpha) || p(alpha)]\n",
    "        kld += td.kl_divergence(q_alpha, alpha_prior)\n",
    "            \n",
    "        # KL[q(zeta) || p(zeta)]\n",
    "        kld += td.kl_divergence(q_zeta, zeta_prior)\n",
    "        \n",
    "        # KL[q(beta_n) || p(beta_n)]\n",
    "        kld += td.kl_divergence(q_beta, beta_prior).sum()\n",
    "        \n",
    "        # KL[q(Omega) || p(Omega)]\n",
    "        kld += q_L_Omega_diag.log_prob(L_Omega_diag).sum() + q_L_Omega_offdiag.log_prob(L_Omega_offdiag).sum() \n",
    "        kld += -L_Sigma_prior.log_prob(L_Sigma).sum() - theta_prior.log_prob(theta).sum()\n",
    "        \n",
    "        # ----- compute ELBO -----\n",
    "        # ELBO = -E[loglik] + KL[q || prior]\n",
    "        elbo = -loglik + kld\n",
    "        \n",
    "        # compute accuracy based on utilities\n",
    "        acc = utilities.argmax(-1) == obs_choices.transpose(0,1)\n",
    "        acc = torch.where(obs_mask.T, acc, acc.new_zeros(()))\n",
    "        acc = acc.sum() / obs_mask.sum()\n",
    "        \n",
    "        # remember values (e.g. to show progress)\n",
    "        self.loglik = loglik\n",
    "        self.kld = kld\n",
    "        self.acc = acc\n",
    "        \n",
    "        return elbo\n",
    "    \n",
    "    \n",
    "    def infer(self, num_epochs=10000, true_alpha=None, true_beta=None, true_beta_resp=None):\n",
    "        \"\"\"\n",
    "        Performs variational inference (amortized variational inference if use_inference_net is set to True). \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_epochs : int, optional\n",
    "            Number of passes/iterations through the dataset to be performed during ELBO maximization (default is 10000).\n",
    "        true_alpha : np.array, optional\n",
    "            Numpy array with true values of the global fixed-effect preference parameters for comparison (useful for investigating the progress of variational inference in cases when the true values of the preference parameters are known). If provided, then this method outputs additional information during ELBO maximization.\n",
    "        true_beta : np.array, optional\n",
    "            Numpy array with true values of the global random-effect preference parameters for comparison (useful for investigating the progress of variational inference in cases when the true values of the preference parameters are known). If provided, then this method outputs additional information during ELBO maximization.\n",
    "        true_beta_resp : np.array, optional\n",
    "            Numpy array with true values of the per-respondent preference parameters for comparison (useful for investigating the progress of variational inference in cases when the true values of the preference parameters are known). If provided, then this method outputs additional information during ELBO maximization.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        results : dict\n",
    "            Python dictionary containing the results of variational inference. \n",
    "        \"\"\"\n",
    "        self.to(self.device)\n",
    "        \n",
    "        #print(\"Initial ELBO value: %.1f\" % self.loss(self.train_x, self.context_info, self.train_y, self.alt_av_mat_cuda, self.mask_cuda, self.alt_ids_cuda).item())\n",
    "\n",
    "        optimizer = Adam(self.parameters(), lr=1e-2)\n",
    "\n",
    "        self.train() # enable training mode\n",
    "        \n",
    "        tic = time.time()\n",
    "        alpha_errors = []\n",
    "        beta_errors = []\n",
    "        betaInd_errors = []\n",
    "        for epoch in range(num_epochs):\n",
    "            permutation = torch.randperm(self.num_resp)\n",
    "            \n",
    "            for i in range(0, self.num_resp, self.batch_size):\n",
    "                \n",
    "                indices = permutation[i:i+self.batch_size]\n",
    "                batch_x, batch_context, batch_y = self.train_x[indices], self.context_info[indices], self.train_y[indices]\n",
    "                batch_alt_av_mat, batch_mask_cuda, batch_alt_ids = self.alt_av_mat_cuda[indices], self.mask_cuda[indices], self.alt_ids_cuda[indices]\n",
    "                \n",
    "                batch_x = batch_x.to(self.device)\n",
    "                batch_context = batch_context.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                batch_alt_av_mat = batch_alt_av_mat.to(self.device)\n",
    "                batch_mask_cuda = batch_mask_cuda.to(self.device)\n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                elbo = self.elbo(batch_x, batch_context, batch_y, batch_alt_av_mat, batch_mask_cuda, batch_alt_ids, indices)\n",
    "\n",
    "                elbo.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if not epoch % 100:\n",
    "                    msg = \"[Epoch %5d] ELBO: %.0f; Loglik: %.0f; Acc.: %.3f\" % (epoch, elbo.item(), self.loglik, self.acc)\n",
    "                    if np.all(true_alpha != None):\n",
    "                        alpha_rmse = np.sqrt(np.mean((true_alpha - self.alpha_mu.detach().cpu().numpy())**2)) if len(true_alpha) > 0 else np.inf\n",
    "                        alpha_errors += [alpha_rmse]\n",
    "                        msg += \"; Alpha RMSE: %.3f\" % (alpha_rmse,)\n",
    "                    if self.dcm_spec.model_type != 'MNL' and np.all(true_beta != None):\n",
    "                        beta_rmse = np.sqrt(np.mean((true_beta - self.zeta_mu.detach().cpu().numpy())**2)) if len(true_beta) > 0 else np.inf\n",
    "                        beta_errors += [beta_rmse]\n",
    "                        msg += \"; Beta RMSE: %.3f\" % (beta_rmse,)\n",
    "                    if self.dcm_spec.model_type != 'MNL' and np.all(true_beta_resp != None):\n",
    "                        params_resps_rmse = np.sqrt(np.mean((true_beta_resp - self.beta_mu.detach().cpu().numpy())**2)) if len(true_beta_resp) > 0 else np.inf\n",
    "                        betaInd_errors += [params_resps_rmse]\n",
    "                        msg += \"; BetaInd RMSE: %.3f\" % (params_resps_rmse,)\n",
    "\n",
    "                    print(msg)\n",
    "\n",
    "        toc = time.time() - tic\n",
    "        print('Elapsed time:', toc, '\\n')\n",
    "            \n",
    "        # prepare python dictionary of results to output\n",
    "        results = {}\n",
    "        results[\"Estimation time\"] = toc\n",
    "        results[\"Est. alpha\"] = self.alpha_mu.detach().cpu().numpy()\n",
    "        if self.dcm_spec.model_type != 'MNL':\n",
    "            results[\"Est. zeta\"] = self.zeta_mu.detach().cpu().numpy()\n",
    "            if self.use_inference_net: \n",
    "                q_alpha, q_zeta, q_L_Sigma_diag, q_L_Sigma_offdiag, q_beta, beta_offsets = self.forward(self.train_x.to(self.device), self.context_info.to(self.device), self.train_y.to(self.device), self.alt_av_mat_cuda.to(self.device), self.alt_ids_cuda.to(self.device))\n",
    "                results[\"Est. beta_n\"] = q_beta.loc.detach().cpu().numpy()\n",
    "            else:\n",
    "                results[\"Est. beta_n\"] = self.beta_mu.detach().cpu().numpy()\n",
    "        results[\"ELBO\"] = elbo.item()\n",
    "        results[\"Loglikelihood\"] = self.loglik.item()\n",
    "        results[\"Accuracy\"] = self.acc.item()\n",
    "        \n",
    "        # show quick summary of results\n",
    "        if np.all(true_alpha != None): print(\"True alpha:\", true_alpha)\n",
    "        print(\"Est. alpha:\", self.alpha_mu.detach().cpu().numpy())\n",
    "        \n",
    "        for i in range(len(self.dcm_spec.fixed_param_names)):\n",
    "            print(\"\\t%s: %.3f\" % (self.dcm_spec.fixed_param_names[i], results[\"Est. alpha\"][i]))\n",
    "        print()\n",
    "\n",
    "        if self.dcm_spec.model_type != 'MNL':\n",
    "            if np.all(true_beta != None): print(\"True zeta:\", true_beta)\n",
    "            print(\"Est. zeta:\", self.zeta_mu.detach().cpu().numpy())\n",
    "        \n",
    "            for i in range(len(self.dcm_spec.mixed_param_names)):\n",
    "                print(\"\\t%s: %.3f\" % (self.dcm_spec.mixed_param_names[i], results[\"Est. zeta\"][i]))\n",
    "            print()\n",
    "\n",
    "        if np.all(true_alpha != None) or np.all(true_beta != None) or np.all(true_beta_resp != None):\n",
    "            if np.all(true_alpha != None): plt.plot(alpha_errors)\n",
    "            if np.all(true_beta != None): plt.plot(beta_errors)\n",
    "            if np.all(true_beta_resp != None): plt.plot(betaInd_errors)\n",
    "            plt.legend(['alpha rmse','beta_rmse','beta_resps_rmse'])\n",
    "            plt.show();\n",
    "        \n",
    "        return results\n",
    "        \n",
    "\n",
    "# instantiate MXL model\n",
    "num_categories = 5\n",
    "mxl = TorchMXL_Ordered(dcm_dataset, num_categories, batch_size=num_resp, use_inference_net=False, use_cuda=True)\n",
    "\n",
    "# True beta: [ 1 -1  1 -1]\n",
    "\n",
    "# run Bayesian inference (variational inference)\n",
    "results = mxl.infer(num_epochs=7000, true_alpha=np.array([1, -1]), true_beta=np.array([1, -1]))\n",
    "\n",
    "# True alpha: [-0.7 -0.2  0.5]\n",
    "\n",
    "mxl.kappa_mu\n",
    "\n",
    "# Rico uses an exp() transform instead of softplus(), so we need to convert to make it comparable\n",
    "torch.log(mxl.softplus(mxl.kappa_mu))\n",
    "\n",
    "The \"results\" dictionary containts a summary of the results of variational inference, including means of the posterior approximations for the different parameters in the model:\n",
    "\n",
    "results\n",
    "\n",
    "This interface is currently being improved to include additional output information, but additional information can be obtained from the attributes of the \"mxl\" object for now. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
